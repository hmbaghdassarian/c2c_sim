{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hratch/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../scripts/')\n",
    "from simulation.graphs import graph_generator as gg_\n",
    "from simulation import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_bias(n, skew):\n",
    "    '''\n",
    "    n: int\n",
    "        Number of entries to bin\n",
    "    skew: extent to which skew binning\n",
    "    '''\n",
    "    unbiased = [1/n]*n\n",
    "    if skew == 0:\n",
    "        biased = np.array(unbiased)\n",
    "    elif skew == 1:\n",
    "        biased = [0]*len(unbiased)\n",
    "        biased[-1] = 1\n",
    "        biased = np.array(biased)\n",
    "    else:\n",
    "        X = np.linspace(0, len(unbiased), len(unbiased))\n",
    "        biased = skewnorm.pdf(X, a = 1, loc = len(unbiased), scale = (1-skew/1)*len(unbiased))\n",
    "        biased = biased/sum(biased)\n",
    "    return biased*100\n",
    "\n",
    "class LR():\n",
    "    '''object to store metadata and relevant information for the ligan-receptor dimension of tensor\n",
    "    for internal use\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, B, ligands, receptors, edge_list, network_type = None, alpha = None, \n",
    "                 fit = None, comp = None, p = None):\n",
    "        '''Initialize\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        B: nx.Graph\n",
    "            a undirected bipartite network representing PPI between ligands and receptors (direction would always be L-->R)\n",
    "        ligands: list \n",
    "            ligand IDs for each protein\n",
    "        receptors: list\n",
    "            receptor IDs for each protein\n",
    "        self.edge_list: list\n",
    "            each entry is a tuple representing a potential interaction between a ligand-receptor pair, ligands on 0 index of each tuple\n",
    "        network_type: str\n",
    "            \"scale-free\" indicates scale-free network, \"normal\" indicates a normal degree distribution\n",
    "        alpha: float\n",
    "            scale-free exponent for network degree distribution (recommended 2<alpha<3)\n",
    "        p: float\n",
    "            probability of adding an edge when using network_type option = 'normal'\n",
    "        fit: igraph.FittedPowerLaw\n",
    "            scale-free network parameters for B_ig (p-value from Kolmogrov-Smirnov test)\n",
    "        comp: pd.DataFrame or None\n",
    "            summary of differences in network properties between  bipartite network and similar Barabasi network\n",
    "        '''\n",
    "        if network_type is None:\n",
    "            self.network_type = 'user-speficied'\n",
    "        else:\n",
    "            self.network_type = network_type\n",
    "        self.B = B\n",
    "        self.ligands = ligands\n",
    "        self.receptors = receptors\n",
    "        self.edge_list = edge_list\n",
    "        self.alpha = alpha\n",
    "        self.fit = fit\n",
    "        self.comp = comp\n",
    "        self.p = p\n",
    "    def generate_metadata(self, n_LR_cats = {2: 0}, cat_skew = 0):\n",
    "        '''Generate metadata groupings for the L-R pairs. Categories are defined as distinct types of \n",
    "        metadata associated with the LR pair, e.g. \"signaling pathway\". Subcategories are\n",
    "        the associated labels within a category, e.g. \"growth\" and \"inflammation\" within the \"signaling pathway\" category.\n",
    "\n",
    "        Note: For skew, 0 means evenly distributed, 1 means all LR pairs fall into the first category/subcategory. \n",
    "\n",
    "        n_LR_cats: dict\n",
    "            The length of the dictionary represents the total number of categories associated with the LR\n",
    "            Each key is an integer representing the number of subcategories for the particular category. \n",
    "            Each value is a float [0,1] indicating the skew of distribution of LRs across \n",
    "            subcategories within each category. \n",
    "        cat_skew: float [0,1]\n",
    "            Skew of distribution of LRs across categories\n",
    "\n",
    "        '''\n",
    "        if len(n_LR_cats) > 1:\n",
    "            raise ValueError('Currently, only one metadata category can be considered')\n",
    "        # group each LR into the categories above\n",
    "        # generate categories\n",
    "        LR_categories = [str(uuid.uuid4()).split('-')[-1] for i in range(len(n_LR_cats))]\n",
    "        cat_bias = weight_bias(n = len(LR_categories), skew = 0)\n",
    "        self.LR_metadata = pd.DataFrame(data = {'LR_id': self.edge_list, \n",
    "                        'category': random.choices(population = LR_categories, weights=cat_bias, \n",
    "                                                  k=len(self.edge_list))})\n",
    "\n",
    "        # generate subcategories\n",
    "        self.LR_metadata['subcategory'] = float('nan')\n",
    "        i = 0\n",
    "        for n_subcat, subcat_skew in n_LR_cats.items():\n",
    "            sub = self.LR_metadata[self.LR_metadata.category == LR_categories[i]]\n",
    "            subcat_bias = weight_bias(n = n_subcat, skew = subcat_skew)\n",
    "            self.LR_metadata.loc[sub.index, 'subcategory'] = random.choices(population = [str(uuid.uuid4()).split('-')[-1] for i in range(n_subcat)], \n",
    "                           weights=subcat_bias, k=sub.shape[0])\n",
    "            i += 1\n",
    "\n",
    "class CCI_MD():\n",
    "    '''Generate the CCI network for the tensor slice at time point 0'''\n",
    "    \n",
    "    def cci_network(self, n_cells, directional = True):\n",
    "            '''Initialize the cell-cell interaction network.\n",
    "\n",
    "            n_cells: int\n",
    "                the total number of cells to simulate; all cell-cell pairs will have a potential interaction, but only\n",
    "                those that actually interact will have a score > 0 in the tensor slice\n",
    "            directional: bool\n",
    "                whether cell-cell interactions are directional (tuple of cell (A,B) indicates interaction from A-->B) or \n",
    "                not\n",
    "\n",
    "            '''\n",
    "            # generate random cell ids\n",
    "            self.cell_ids = [str(uuid.uuid4()).split('-')[-1] for i in range(n_cells)]\n",
    "            if directional:\n",
    "                self.cell_interactions = list(itertools.permutations(self.cell_ids, 2))\n",
    "            else:\n",
    "                self.cell_interactions = list(itertools.combinations(self.cell_ids, 2))\n",
    "    def generate_metadata(self, n_cell_cats = {2: 0}, cat_skew = 0, \n",
    "                         remove_homotypic = None):\n",
    "        '''Generate metadata groupings for the cells (individual). Categories are defined as distinct types of \n",
    "        metadata associated with the cell or protein, e.g. \"cell type\" and \"cell cycle phase\". Subcategories are\n",
    "        the associated labels within a category, e.g. \"T-cell\" and \"dendritic cell\" within the \"cell type\" category.\n",
    "        \n",
    "        Note: For skew, 0 means evenly distributed, 1 means all cells fall into the first category/subcategory. \n",
    "        \n",
    "        n_cell_cats: dict\n",
    "            The length of the dictionary represents the total number of categories associated with the cell\n",
    "            Each key is an integer representing the number of subcategories for the particular category. \n",
    "            Each value is a float [0,1] indicating the skew of distribution of cells across \n",
    "            subcategories within each category. \n",
    "        cat_skew: float [0,1]\n",
    "            Skew of distribution of cells across categories\n",
    "        remove_homotypic: int\n",
    "            whether to remove homotypic ineractions between cells by cell category; how many categories to consider; \n",
    "            must be <= the number of categories present in the metadata\n",
    "\n",
    "        '''\n",
    "        if len(n_cell_cats) > 1:\n",
    "            raise ValueError('Currently, only one metadata category can be considered')\n",
    "        if remove_homotypic > len(n_cell_cats):\n",
    "            raise ValueError('The value for \"remove_homotypic\" cannot be larger than the total number of categories associated with the cells')\n",
    "\n",
    "        # group each cell into the categories above\n",
    "        # generate categories\n",
    "        cell_categories = [str(uuid.uuid4()).split('-')[-1] for i in range(len(n_cell_cats))]\n",
    "        cat_bias = weight_bias(n = len(cell_categories), skew = 0)\n",
    "        self.cell_ids = pd.DataFrame(data = {'cell_id': self.cell_ids, \n",
    "                        'category': random.choices(population = cell_categories, weights=cat_bias, \n",
    "                                                  k=len(self.cell_ids))})\n",
    "        \n",
    "        # generate subcategories\n",
    "        self.cell_ids['subcategory'] = float('nan')\n",
    "        i = 0\n",
    "        for n_subcat, subcat_skew in n_cell_cats.items():\n",
    "            sub = self.cell_ids[self.cell_ids.category == cell_categories[i]]\n",
    "            subcat_bias = weight_bias(n = n_subcat, skew = subcat_skew)\n",
    "            self.cell_ids.loc[sub.index, 'subcategory'] = random.choices(population = [str(uuid.uuid4()).split('-')[-1] for i in range(n_subcat)], \n",
    "                           weights=subcat_bias, k=sub.shape[0])\n",
    "            i += 1\n",
    "        self.cell_metadata = self.cell_ids\n",
    "        del self.cell_ids\n",
    "        \n",
    "\n",
    "        if remove_homotypic is not None and remove_homotypic > 0: # remove homotypic interactions of a given category\n",
    "            print('Remove homotypic cell interactions for {} categories'.format(remove_homotypic))\n",
    "            i = 0\n",
    "            to_remove = list()\n",
    "            while i < remove_homotypic:\n",
    "                cat = cell_categories[i]\n",
    "                sub = self.cell_metadata[self.cell_metadata.category == cell_categories[i]]\n",
    "                cell_ids = sub.cell_id.tolist()\n",
    "\n",
    "                for ccp in self.cell_interactions:\n",
    "                    sub_ = sub[(sub.cell_id == ccp[0]) | (sub.cell_id == ccp[1])]\n",
    "                    if sub_.shape[0] == 2 and sub_.subcategory.unique().shape[0] == 1:\n",
    "                        to_remove.append(ccp)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            # remove homotypic interactions as identified above for categories 1-i\n",
    "            self.cell_interactions = list(set(self.cell_interactions).difference(to_remove))\n",
    "\n",
    "            # filter out any cells that no longer are present \n",
    "            cell_ids = list(set(sum(list(zip(*self.cell_interactions)), ())))\n",
    "            self.cell_metadata = self.cell_metadata[self.cell_metadata.cell_id.isin(cell_ids)]\n",
    "            self.cell_metadata.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate():\n",
    "    def __init__(self):\n",
    "        '''Initialize self\n",
    "\n",
    "        '''\n",
    "        self.cci = None\n",
    "    \n",
    "    def LR_network(self, network_type = None, B = None, subset = False, **params):\n",
    "        '''\n",
    "        Simulates a PPI network of *potential* ligand-receptor interactions, or extracts information. \\\n",
    "        from a use provided network.Defines one tensor dimension\n",
    "        Caveats: for a scale-free network, the number of ligands = the number of receptors \\\n",
    "                 for either network, there may be disconnected edges depending on \"p\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        network_type: str\n",
    "             \"scale-free\" to generate a scale-free network or \"normal\" to generate a network with a normal degree distribution\n",
    "        B: nx.Graph\n",
    "            a user provided undirected, unweighted bipartite network. Assumes in B.nodes, ligands are listed \\\n",
    "            before receptors. Takes precedence over network_type.\n",
    "        subset: bool\n",
    "            if B is provided and subset is true, this will take a random subset of the network, dropping disconnected nodes \\\n",
    "            (of a specified size, specfied in params)\n",
    "        **params: dict (keys for each option specified below)\n",
    "            the required parameters for generating a bipartite, undirected random network either scale-free or not. \\\n",
    "            \n",
    "            network_type = scale-free: keys - nodes, degrees, alpha, edges (see graphs.graph_generator.bipartite_sf for description) \n",
    "            network_type = normal: keys - n_ligands, n_receptors, p analogous to n,m,p in nx.bipartite.gnmk_random_graph\n",
    "            B != None: keys - n_ligands as described above\n",
    "            subset = True: keys - \n",
    "                n_ligands as described above\n",
    "                'subset_size' a value between (0,1) indicating the proportional \\\n",
    "                size of the subset (by nodes) compared to the network\n",
    "                'subset_type' either 'edges' or 'nodes' indicating whether to subset by removing nodes or edges \\\n",
    "                (edges recommended because they maintain the scale-free property)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self.LR: \n",
    "            populates LR object, key outputs outlined here\n",
    "        self.LR.B: nx.Graph\n",
    "            undirected bipartite graph with specified degree distribution (power or normal), or user specified B \\\n",
    "            disconnected nodes are removed\n",
    "        self.LR.edge_list: list\n",
    "            each entry is a tuple representing a potential interaction between a ligand-receptor pair, ligands on 0 index of each tuple\n",
    "\n",
    "\n",
    "        '''\n",
    "        gg = gg_() # return networkx object for graphs\n",
    "        user = False\n",
    "        if B is not None: #untested\n",
    "            user = True\n",
    "            # properties checked when calling gg.nx_to_edgelist\n",
    "            if network_type is not None:\n",
    "                warnings.warn('You have specified a network type and provided a network, B will take priority over network type')\n",
    "            if 'n_ligands' not in params:\n",
    "                raise ValueError('For a provided B, you must specify n_ligands in params')\n",
    "            \n",
    "            if subset:\n",
    "                if 'subset_size' not in params or 'subset_type' not in params:\n",
    "                    raise ValueError('To subset B, you must provide a desired subset_size and subset_type')\n",
    "                if params['subset_type'] == 'edges':\n",
    "                    B = gg.subset_edges(B, subset_size = params['subset_size'], drop = True)\n",
    "                elif params['subset_type'] == 'nodes': \n",
    "                    B = gg.subset_nodes(B, subset_size = params['subset_size'], drop = True)\n",
    "                else:\n",
    "                    raise ValueError(\"The subset_type param must be either 'edges' or 'nodes'\")\n",
    "            \n",
    "            \n",
    "        elif network_type == 'scale-free': \n",
    "            if 'degrees' not in params or 'nodes' not in params:\n",
    "                raise ValueError('Must specify degrees and nodes in **params')\n",
    "            if 'alpha' not in params: \n",
    "                params['alpha'] = 2 # also default in gg obj, didn't make it a **kwrag\n",
    "            if 'edges' not in params:\n",
    "                B, node_groups, fit, comp = gg.bipartite_sf(nodes = params['nodes'], degrees = params['degrees'], \n",
    "                                                            alpha = params['alpha'])\n",
    "            else:\n",
    "                B, node_groups, fit, comp = gg.bipartite_sf(nodes = params['nodes'], degrees = params['degrees'], \n",
    "                                                        alpha = params['alpha'], edges = ['edges'])  \n",
    "            B = B['nx']\n",
    "            params['n_ligands'] = params['nodes'] # same no. of ligands and receptors\n",
    "        elif network_type == 'normal':\n",
    "            if sorted(params) != ['n_ligands', 'n_receptors', 'p']:\n",
    "                raise ValueError('Must specify n_ligands, n_receptors in **params')\n",
    "            else:\n",
    "                B = nx.bipartite.random_graph(params['n_ligands'],params['n_receptors'], params['p'])\n",
    "        else:\n",
    "            raise ValueError('Must specify an appropriate network_type or provide a network B')\n",
    "        \n",
    "        B, edge_list, ng = gg.nx_to_edgelist(B, params['n_ligands']) # formatting/extract info\n",
    "        \n",
    "        # store PPI information in LR()\n",
    "        if user:\n",
    "            self.LR = LR(B, ng['1'], ng['2'], edge_list)\n",
    "        elif network_type == 'scale-free':\n",
    "            self.LR = LR(B, ng['1'], ng['2'], edge_list, network_type = network_type, \n",
    "                         alpha = params['alpha'], fit = fit, comp = comp)\n",
    "        elif network_type == 'normal':\n",
    "            self.LR = LR(B, ng['1'], ng['2'], edge_list, network_type = network_type, p = params['p'])\n",
    "\n",
    "    def emulate_sf_network(self, G):\n",
    "        '''Emulate a user-provided (recommended scale-free) network for L-R pair tensors dimension\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        G: nx.Graph\n",
    "            user-provided network (recommended scale-free)\n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        G2: nx.Graph\n",
    "            random bipartite scale-free network built using G's properties\n",
    "        \n",
    "        '''\n",
    "        gg = gg_()\n",
    "        fit = gg.power_fit(G)\n",
    "        if fit.p < 0.05:\n",
    "            warnings.warn('Input network is not scale-free')\n",
    "        print('----Simulated network------')\n",
    "        \n",
    "        G2, node_groups, fit2, comp = gg.bipartite_sf(nodes = round(len(G.nodes)), # should be 1/2 the nodes, but many are disconnected \n",
    "                                 degrees = np.median([i[1] for i in G.degree]),\n",
    "                                 alpha = fit.alpha, edges = len(G.edges),\n",
    "                                 check_properties = True, compare_barabasi = False)\n",
    "        G2 = G2['nx']\n",
    "        gg.drop_disconnected_nodes(G2)\n",
    "        return G2\n",
    "    \n",
    "    def _generate_clrm(self, n_patterns):\n",
    "        '''Generates cell-LR metadata pairs for tensor slices. See tensor_slice_t0 method for argument descriptions'''\n",
    "        n_lr_cat = len(self.LR.LR_metadata.subcategory.unique())\n",
    "        n_cc_cat = len(self.LR.LR_metadata.subcategory.unique())\n",
    "\n",
    "        # all possible groups that have patterns of expression\n",
    "\n",
    "        lr_group = list()\n",
    "        for i in range(1, n_lr_cat + 3):\n",
    "            lr_group += list(itertools.combinations(self.LR.LR_metadata.subcategory.unique(), i))\n",
    "\n",
    "        # all possible cell groups that have patterns of expression\n",
    "        ccat_map = dict(zip(self.cci.cell_metadata.cell_id, self.cci.cell_metadata.subcategory))\n",
    "        ccati = list()\n",
    "        for ci in self.cci.cell_interactions:\n",
    "            ccati.append((ccat_map[ci[0]], ccat_map[ci[1]]))\n",
    "        ccati = pd.Series(ccati).unique()\n",
    "\n",
    "        # all possible groups of cell metadata - LR metadata pairs\n",
    "        clrm = pd.DataFrame(columns = ['cell_subcat', 'LR_subcat'])\n",
    "        counter = 0\n",
    "        for i in list(itertools.product(ccati, lr_group)):\n",
    "            clrm.loc[counter, : ]= [i[0], i[1]]\n",
    "            counter += 1\n",
    "\n",
    "        # no all-all combinations\n",
    "        # clrm.drop(index = [clrm.shape[0] - 1], inplace = True)\n",
    "\n",
    "        if n_patterns > clrm.shape[0]:\n",
    "            warnings.warn('More patterns than possible specificed, setting to maximum possible: {}'.format(clrm.shape[0]))\n",
    "            n_patterns = clrm.shape[0]\n",
    "\n",
    "        for i in range(n_patterns):\n",
    "            clrm = clrm.loc[sorted(random.sample(clrm.index.tolist(), k = n_patterns)),]\n",
    "        clrm.reset_index(inplace = True, drop = True)\n",
    "        clrm.LR_subcat = clrm.LR_subcat.apply(lambda x: x[0])\n",
    "        \n",
    "        self.clrm = clrm\n",
    "    \n",
    "    def _generate_t0(self, noise, binary):\n",
    "        '''Generate tensor slice 0. See tensor_slice_t0 method for agrument descriptions'''\n",
    "        # initialize\n",
    "        self.ts0 = pd.DataFrame(columns = self.cci.cell_interactions, index = self.LR.edge_list)\n",
    "\n",
    "        #------------------------------------------------------------------------------------------\n",
    "\n",
    "        # sort metadata categories\n",
    "        ccat_map = dict(zip(self.cci.cell_metadata.cell_id, self.cci.cell_metadata.subcategory))\n",
    "        LR_map = dict(zip(self.LR.LR_metadata.LR_id, self.LR.LR_metadata.subcategory))\n",
    "        lrcats = [LR_map[lri] for lri in self.ts0.index]\n",
    "        ccats = [(ccat_map[ci[0]], ccat_map[ci[1]]) for ci in self.ts0.columns]\n",
    "\n",
    "        self.clrm[['mean']] = list(np.arange(1/self.clrm.shape[0], 1, 1/self.clrm.shape[0])) + [1]\n",
    "\n",
    "        # background\n",
    "        if noise == 0:\n",
    "            self.ts0.fillna(0, inplace = True)\n",
    "        else:\n",
    "            if not binary: # generate background noise values\n",
    "                # noise values increase with noise, and have a maximum average of self.clrm['mean'].min()\n",
    "                scale = self.clrm['mean'].min()/np.array([utils.piecewise_fit(self.clrm['mean'].min(), *utils.fit_params)])[0]\n",
    "                vals = utils.get_truncated_normal(n = self.ts0.shape[0]*self.ts0.shape[1], \n",
    "                                                  sd = noise*self.clrm['mean'].min(), mean = 0)*scale\n",
    "                self.ts0[:] = vals.reshape(self.ts0.shape)\n",
    "            else:\n",
    "                raise ValueError('Binary situations not yet dealt with')\n",
    "        #        freq = np.mean([abs(i) if abs(i) <= 1 else 1 for i in np.random.normal(loc = 0, scale = noise*0.33, size = 10**5)])\n",
    "        #                 background_coord_noise = random.sample(background_coord, k = int(round(freq*len(background_coord))))\n",
    "        #                 for coord in background_coord_noise:\n",
    "        #                     self.ts0.iloc[coord] = 1\n",
    "\n",
    "        #------------------------------------------------------------------------------------------\n",
    "\n",
    "        # add CCI values by cell-LR metadata pairs\n",
    "\n",
    "        # get tensor slice coordinates for CC-LR pairs with expected patterns\n",
    "        def get_coords(i):\n",
    "            coords = list(zip([k for k in range(len(ccats)) if \\\n",
    "                                         ccats[k] == self.clrm.loc[i, 'cell_subcat']], \n",
    "                                         [k for k in range(len(lrcats)) if lrcats[k] == self.clrm.loc[i, 'LR_subcat']]))\n",
    "            return [tuple([i[0] for i in coords]), tuple([i[1] for i in coords])]\n",
    "\n",
    "        self.clrm['ts_coordinates'] = pd.Series(self.clrm.index).apply(lambda i: get_coords(i)).tolist()\n",
    "        # add values\n",
    "        for i in self.clrm.index:\n",
    "            avg_val = self.clrm.loc[i, 'mean']\n",
    "            coords = self.clrm.loc[i, 'ts_coordinates']\n",
    "            if not binary: \n",
    "                self.ts0.values[coords] = avg_val if noise == 0 else \\\n",
    "                                          utils.get_truncated_normal(n = len(coords[0]), sd = noise*avg_val, mean = avg_val)\n",
    "            else:\n",
    "                raise ValueError('Binary situations not yet dealt with')\n",
    "        #         adj_coord = random.sample(coords, int(round(len(coords)*self.clrm.loc[i, 'mean'])))\n",
    "        #         for coord in adj_coord:\n",
    "        #             self.ts0.iloc[coord] = 1\n",
    "        #         if noise > 0:\n",
    "        #             cm = {0:1, 1:0}\n",
    "        #             change_coords = random.sample(coords, int(round(len(coords)*self.clrm.loc[i, 'mean']*noise*0.5)))\n",
    "        #             for coord in change_coords:\n",
    "        #                 self.ts0.iloc[coord] = cm[self.ts0.iloc[coord]]\n",
    "    \n",
    "    def tensor_slice_t0(self, noise = 0, n_patterns = 2, binary = False):\n",
    "\n",
    "        '''Simulates a static time point tensor slice\n",
    "        \n",
    "        *Note, in current format, only one cell-cell metadata subcategory can have an interaction pattern \n",
    "        ie, can't combine multiple cell-cell pairs to have the same pattern\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_patterns: int (> 0)\n",
    "            the number of cell metadata - LR metadata pairs for which to form distinct interactions \n",
    "            the remainder will default to 0, with noise increasing this value\n",
    "            the groups with distinct interactions will each have distincts values\n",
    "            recommended to set decomposition rank = n_patterns\n",
    "        binary: bool\n",
    "            whether L-R scores are binary or continuous b/w [0,1]\n",
    "        noise: float [0,1]\n",
    "            the amount of noise to add to the data\n",
    "            as noise increases, the fraction of noisy cc-lr coordinates increases, as does the change in value of the interaction, and the standard deviation if non-continuous\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.ts0: pd.DataFrame\n",
    "            matrix with cell network_type pairs as columns, ligand-receptor pairs as rows, scores as entries\n",
    "        '''\n",
    "        \n",
    "        if sim.cci is None or type(sim.cci) != CCI_MD:\n",
    "            raise ValueError('Make sure to generate cell-cell network and metadata with the CCI_MD() class')\n",
    "        if noise > 1 or noise < 0: \n",
    "            raise ValueError('Noise must be between 0 and 1')\n",
    "        if type(binary) is not bool:\n",
    "            raise ValueError('binary arg must be boolean')\n",
    "        \n",
    "        # by separating into two methods, can test different values of noise without changing category pairings\n",
    "        self._generate_clrm(n_patterns = n_patterns)\n",
    "        self._generate_t0(noise = noise, binary = binary)\n",
    "            \n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hratch/Projects/cci_dt/notebooks/simulation/tmp3ul2cj0p_bipartite_sf.csv\n",
      "Generate undirected, bipartite, scale-free graph\n",
      "Check network properties\n",
      "All properties are as expected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../scripts/simulation/graphs.py:164: UserWarning: 988 nodes are disconnected, removing from network\n",
      "  warnings.warn(mssg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove homotypic cell interactions for 1 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hratch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:220: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "sim_sf = Simulate() \n",
    "sim_norm = Simulate()\n",
    "\n",
    "# simulate a randomly connected ligand-receptor network (potential interactions)\n",
    "sim_sf.LR_network(network_type = 'scale-free', **{'nodes': 1000, 'degrees': 3, 'alpha': 2}) #scale-free\n",
    "sim_norm.LR_network(network_type = 'normal', **{'n_ligands': 500, 'n_receptors': 500, 'p': 0.5}) # normally distributed\n",
    "# from here on proceed with the scale-free network\n",
    "sim = sim_sf\n",
    "\n",
    "# LR metadata\n",
    "sim.LR.generate_metadata(n_LR_cats = {3: 0}, cat_skew = 0)\n",
    "\n",
    "# cell metadata\n",
    "cci = CCI_MD()\n",
    "cci.cci_network(n_cells = 50, directional = False)\n",
    "# generate 1 metadata categories, with 3 subcategories and 0 skew, the overall skew of categories is 0\n",
    "cci.generate_metadata(n_cell_cats = {3: 0}, cat_skew = 0, remove_homotypic = 1)\n",
    "# add metadata to simulation object\n",
    "sim.cci = cci\n",
    "\n",
    "# save for visualization of slice\n",
    "for_viz = sim.copy()\n",
    "\n",
    "\n",
    "#generate a t0 tensor slice with continuous LR scores, baseline noise, and patterns for 4 cell-LR metadat subcategory pairs\n",
    "sim.tensor_slice_t0(noise = 0.05, n_patterns = 4, binary = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_subcat</th>\n",
       "      <th>LR_subcat</th>\n",
       "      <th>mean</th>\n",
       "      <th>ts_coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(708c9f8643d2, fc36fd7eda8a)</td>\n",
       "      <td>a729f9ded85c</td>\n",
       "      <td>0.25</td>\n",
       "      <td>[(0, 13, 18, 21, 24, 40, 46, 50, 63, 64, 73, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(708c9f8643d2, 665a3d5c6126)</td>\n",
       "      <td>a729f9ded85c</td>\n",
       "      <td>0.50</td>\n",
       "      <td>[(4, 5, 7, 20, 32, 38, 41, 42, 43, 44, 45, 49,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(665a3d5c6126, fc36fd7eda8a)</td>\n",
       "      <td>a729f9ded85c</td>\n",
       "      <td>0.75</td>\n",
       "      <td>[(6, 9, 12, 29, 30, 48, 51, 68, 116, 130, 131,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(665a3d5c6126, fc36fd7eda8a)</td>\n",
       "      <td>a729f9ded85c</td>\n",
       "      <td>1.00</td>\n",
       "      <td>[(6, 9, 12, 29, 30, 48, 51, 68, 116, 130, 131,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cell_subcat     LR_subcat  mean  \\\n",
       "0  (708c9f8643d2, fc36fd7eda8a)  a729f9ded85c  0.25   \n",
       "1  (708c9f8643d2, 665a3d5c6126)  a729f9ded85c  0.50   \n",
       "2  (665a3d5c6126, fc36fd7eda8a)  a729f9ded85c  0.75   \n",
       "3  (665a3d5c6126, fc36fd7eda8a)  a729f9ded85c  1.00   \n",
       "\n",
       "                                      ts_coordinates  \n",
       "0  [(0, 13, 18, 21, 24, 40, 46, 50, 63, 64, 73, 7...  \n",
       "1  [(4, 5, 7, 20, 32, 38, 41, 42, 43, 44, 45, 49,...  \n",
       "2  [(6, 9, 12, 29, 30, 48, 51, 68, 116, 130, 131,...  \n",
       "3  [(6, 9, 12, 29, 30, 48, 51, 68, 116, 130, 131,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delta_condition(self, n_conditions, patterns = None):\n",
    "    '''Generate tensors across n_conditions, with changing interaction patterns for each LR-CC metadat pair in \n",
    "    self.clrm\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_conditions: int (> 2)\n",
    "        the number of cell metadata - LR metadata pairs for which to form distinct interactions \n",
    "        the remainder will default to 0, with noise increasing this value\n",
    "        the groups with distinct interactions will each have distincts values\n",
    "        recommended to set decomposition rank = n_patterns\n",
    "    binary: bool\n",
    "        whether L-R scores are binary or continuous b/w [0,1]\n",
    "    noise: float [0,1]\n",
    "        the amount of noise to add to the data\n",
    "        as noise increases, the fraction of noisy cc-lr coordinates increases, as does the change in value of the interaction, and the standard deviation if non-continuous\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self.ts0: pd.DataFrame\n",
    "        matrix with cell network_type pairs as columns, ligand-receptor pairs as rows, scores as entries\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if n_conditions <=2:\n",
    "        warnings.warn('At least 4 conditions are required')\n",
    "        n_conditions = 3\n",
    "        \n",
    "    allowed_patterns = ['pulse', 'linear', 'exponential', 'oscillate']\n",
    "    if patterns is not None:\n",
    "        if len(set(patterns).difference(allowed_patterns)) > 0:\n",
    "            raise ValueError('Patterns can only include: ' + ', '.join(allowed_patterns))\n",
    "    else:\n",
    "        patterns = allowed_patterns\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "patterns = ['pulse', 'linear', 'exponential', 'oscillate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['pulse', 'linear', 'exponential', 'oscillate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pulse', 'linear', 'exponential', 'oscillate']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0514beb4e48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclrm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "self.clrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exponential', 'oscillate', 'pulse', 'linear']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cci_dt] *",
   "language": "python",
   "name": "conda-env-cci_dt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
