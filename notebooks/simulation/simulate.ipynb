{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hratch/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorly\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../scripts/')\n",
    "from simulation.graphs import graph_generator as gg_\n",
    "from simulation import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_bias(n, skew):\n",
    "    '''\n",
    "    n: int\n",
    "        Number of entries to bin\n",
    "    skew: extent to which skew binning\n",
    "    '''\n",
    "    unbiased = [1/n]*n\n",
    "    if skew == 0:\n",
    "        biased = np.array(unbiased)\n",
    "    elif skew == 1:\n",
    "        biased = [0]*len(unbiased)\n",
    "        biased[-1] = 1\n",
    "        biased = np.array(biased)\n",
    "    else:\n",
    "        X = np.linspace(0, len(unbiased), len(unbiased))\n",
    "        biased = skewnorm.pdf(X, a = 1, loc = len(unbiased), scale = (1-skew/1)*len(unbiased))\n",
    "        biased = biased/sum(biased)\n",
    "    return biased*100\n",
    "\n",
    "class LR():\n",
    "    '''object to store metadata and relevant information for the ligan-receptor dimension of tensor\n",
    "    for internal use\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, B, ligands, receptors, edge_list, network_type = None, alpha = None, \n",
    "                 fit = None, comp = None, p = None):\n",
    "        '''Initialize\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        B: nx.Graph\n",
    "            a undirected bipartite network representing PPI between ligands and receptors (direction would always be L-->R)\n",
    "        ligands: list \n",
    "            ligand IDs for each protein\n",
    "        receptors: list\n",
    "            receptor IDs for each protein\n",
    "        self.edge_list: list\n",
    "            each entry is a tuple representing a potential interaction between a ligand-receptor pair, ligands on 0 index of each tuple\n",
    "        network_type: str\n",
    "            \"scale-free\" indicates scale-free network, \"normal\" indicates a normal degree distribution\n",
    "        alpha: float\n",
    "            scale-free exponent for network degree distribution (recommended 2<alpha<3)\n",
    "        p: float\n",
    "            probability of adding an edge when using network_type option = 'normal'\n",
    "        fit: igraph.FittedPowerLaw\n",
    "            scale-free network parameters for B_ig (p-value from Kolmogrov-Smirnov test)\n",
    "        comp: pd.DataFrame or None\n",
    "            summary of differences in network properties between  bipartite network and similar Barabasi network\n",
    "        '''\n",
    "        if network_type is None:\n",
    "            self.network_type = 'user-speficied'\n",
    "        else:\n",
    "            self.network_type = network_type\n",
    "        self.B = B\n",
    "        self.ligands = ligands\n",
    "        self.receptors = receptors\n",
    "        self.edge_list = edge_list\n",
    "        self.alpha = alpha\n",
    "        self.fit = fit\n",
    "        self.comp = comp\n",
    "        self.p = p\n",
    "    def generate_metadata(self, n_LR_cats = {2: 0}, cat_skew = 0):\n",
    "        '''Generate metadata groupings for the L-R pairs. Categories are defined as distinct types of \n",
    "        metadata associated with the LR pair, e.g. \"signaling pathway\". Subcategories are\n",
    "        the associated labels within a category, e.g. \"growth\" and \"inflammation\" within the \"signaling pathway\" category.\n",
    "\n",
    "        Note: For skew, 0 means evenly distributed, 1 means all LR pairs fall into the first category/subcategory. \n",
    "\n",
    "        n_LR_cats: dict\n",
    "            The length of the dictionary represents the total number of categories associated with the LR\n",
    "            Each key is an integer representing the number of subcategories for the particular category. \n",
    "            Each value is a float [0,1] indicating the skew of distribution of LRs across \n",
    "            subcategories within each category. \n",
    "        cat_skew: float [0,1]\n",
    "            Skew of distribution of LRs across categories\n",
    "\n",
    "        '''\n",
    "        if len(n_LR_cats) > 1:\n",
    "            raise ValueError('Currently, only one metadata category can be considered')\n",
    "        # group each LR into the categories above\n",
    "        # generate categories\n",
    "        LR_categories = [str(uuid.uuid4()).split('-')[-1] for i in range(len(n_LR_cats))]\n",
    "        cat_bias = weight_bias(n = len(LR_categories), skew = 0)\n",
    "        self.LR_metadata = pd.DataFrame(data = {'LR_id': self.edge_list, \n",
    "                        'category': random.choices(population = LR_categories, weights=cat_bias, \n",
    "                                                  k=len(self.edge_list))})\n",
    "\n",
    "        # generate subcategories\n",
    "        self.LR_metadata['subcategory'] = float('nan')\n",
    "        i = 0\n",
    "        for n_subcat, subcat_skew in n_LR_cats.items():\n",
    "            sub = self.LR_metadata[self.LR_metadata.category == LR_categories[i]]\n",
    "            subcat_bias = weight_bias(n = n_subcat, skew = subcat_skew)\n",
    "            self.LR_metadata.loc[sub.index, 'subcategory'] = random.choices(population = [str(uuid.uuid4()).split('-')[-1] for i in range(n_subcat)], \n",
    "                           weights=subcat_bias, k=sub.shape[0])\n",
    "            i += 1\n",
    "\n",
    "class CCI_MD():\n",
    "    '''Generate the CCI network for the tensor slice at time point 0'''\n",
    "    \n",
    "    def cci_network(self, n_cells, directional = True, autocrine = True):\n",
    "            '''Initialize the cell-cell interaction network.\n",
    "\n",
    "            n_cells: int\n",
    "                the total number of cells to simulate; all cell-cell pairs will have a potential interaction, but only\n",
    "                those that actually interact will have a score > 0 in the tensor slice\n",
    "            directional: bool\n",
    "                whether cell-cell interactions are directional (tuple of cell (A,B) indicates interaction from A-->B) or \n",
    "                not\n",
    "            autocrine: bool\n",
    "                whether cells can interact with themselves\n",
    "            '''\n",
    "            # generate random cell ids\n",
    "            self.cell_ids = [str(uuid.uuid4()).split('-')[-1] for i in range(n_cells)]\n",
    "            if directional:\n",
    "                self.cell_interactions = list(itertools.permutations(self.cell_ids, 2))\n",
    "            else:\n",
    "                self.cell_interactions = list(itertools.combinations(self.cell_ids, 2))\n",
    "            \n",
    "            if autocrine:\n",
    "                self.cell_interactions += [(id_, id_) for id_ in self.cell_ids]\n",
    "    def generate_metadata(self, n_cell_cats = {2: 0}, cat_skew = 0, \n",
    "                         remove_homotypic = None):\n",
    "        '''Generate metadata groupings for the cells (individual). Categories are defined as distinct types of \n",
    "        metadata associated with the cell or protein, e.g. \"cell type\" and \"cell cycle phase\". Subcategories are\n",
    "        the associated labels within a category, e.g. \"T-cell\" and \"dendritic cell\" within the \"cell type\" category.\n",
    "        \n",
    "        Note: For skew, 0 means evenly distributed, 1 means all cells fall into the first category/subcategory. \n",
    "        \n",
    "        n_cell_cats: dict\n",
    "            The length of the dictionary represents the total number of categories associated with the cell\n",
    "            Each key is an integer representing the number of subcategories for the particular category. \n",
    "            Each value is a float [0,1] indicating the skew of distribution of cells across \n",
    "            subcategories within each category. \n",
    "        cat_skew: float [0,1]\n",
    "            Skew of distribution of cells across categories\n",
    "        remove_homotypic: int\n",
    "            whether to remove homotypic ineractions between cells by cell category; how many categories to consider; \n",
    "            must be <= the number of categories present in the metadata. Recommended to leave as default and \n",
    "            instead toggle consider_homotypic option in Simulate().generate_tensor_md()\n",
    "\n",
    "        '''\n",
    "        if len(n_cell_cats) > 1:\n",
    "            raise ValueError('Currently, only one metadata category can be considered')\n",
    "        if remove_homotypic > len(n_cell_cats):\n",
    "            raise ValueError('The value for \"remove_homotypic\" cannot be larger than the total number of categories associated with the cells')\n",
    "\n",
    "        # group each cell into the categories above\n",
    "        # generate categories\n",
    "        cell_categories = [str(uuid.uuid4()).split('-')[-1] for i in range(len(n_cell_cats))]\n",
    "        cat_bias = weight_bias(n = len(cell_categories), skew = 0)\n",
    "        self.cell_ids = pd.DataFrame(data = {'cell_id': self.cell_ids, \n",
    "                        'category': random.choices(population = cell_categories, weights=cat_bias, \n",
    "                                                  k=len(self.cell_ids))})\n",
    "        \n",
    "        # generate subcategories\n",
    "        self.cell_ids['subcategory'] = float('nan')\n",
    "        i = 0\n",
    "        for n_subcat, subcat_skew in n_cell_cats.items():\n",
    "            sub = self.cell_ids[self.cell_ids.category == cell_categories[i]]\n",
    "            subcat_bias = weight_bias(n = n_subcat, skew = subcat_skew)\n",
    "            self.cell_ids.loc[sub.index, 'subcategory'] = random.choices(population = [str(uuid.uuid4()).split('-')[-1] for i in range(n_subcat)], \n",
    "                           weights=subcat_bias, k=sub.shape[0])\n",
    "            i += 1\n",
    "        self.cell_metadata = self.cell_ids\n",
    "        del self.cell_ids\n",
    "        \n",
    "\n",
    "        if remove_homotypic is not None and remove_homotypic > 0: # remove homotypic interactions of a given category\n",
    "            print('Remove homotypic cell interactions for {} categories'.format(remove_homotypic))\n",
    "            i = 0\n",
    "            to_remove = list()\n",
    "            while i < remove_homotypic:\n",
    "                cat = cell_categories[i]\n",
    "                sub = self.cell_metadata[self.cell_metadata.category == cell_categories[i]]\n",
    "                cell_ids = sub.cell_id.tolist()\n",
    "\n",
    "                for ccp in self.cell_interactions:\n",
    "                    sub_ = sub[(sub.cell_id == ccp[0]) | (sub.cell_id == ccp[1])]\n",
    "                    if sub_.shape[0] == 2 and sub_.subcategory.unique().shape[0] == 1:\n",
    "                        to_remove.append(ccp)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            # remove homotypic interactions as identified above for categories 1-i\n",
    "            self.cell_interactions = list(set(self.cell_interactions).difference(to_remove))\n",
    "\n",
    "            # filter out any cells that no longer are present \n",
    "            cell_ids = list(set(sum(list(zip(*self.cell_interactions)), ())))\n",
    "            self.cell_metadata = self.cell_metadata[self.cell_metadata.cell_id.isin(cell_ids)]\n",
    "            self.cell_metadata.reset_index(inplace = True, drop = True)\n",
    "\n",
    "class sim_tensor():\n",
    "    '''Class to consolidate and store all label information for the input of tensor-cell2cell'''\n",
    "    def __init__(self, tensor_cci, conditions, lr_labels, senders, receivers, \n",
    "                lr_metadata, cell_metadata, cell_lr_metadata):\n",
    "        '''\n",
    "        Note: object stores all input arguments + self.rank (see below)\n",
    "        \n",
    "        Paramters\n",
    "        -------\n",
    "        tensor_cci: tensorly.tensor\n",
    "            A tensor formatted in the method needed to run the tensor-cell2cell pipeline\n",
    "        conditions: list\n",
    "            label for the conditions, defines first dimension of tensor\n",
    "        lr_labels: list \n",
    "            label for the LR labels, defines the second dimension of tensor\n",
    "        sender: list\n",
    "            label for sender cells, defines the third dimension of the tensor\n",
    "        receiver: list\n",
    "            label for sender cells, defines the fourth dimension of the tensor\n",
    "        lr_metadata: pd.DataFrame\n",
    "            metadata associated with the ligand-receptor pairs\n",
    "        cell_metadata: pd.DataFrame\n",
    "            metadata associated with the cells\n",
    "        cell_lr_metadata: pd.DataFrame\n",
    "            metadata associated with the expected groups of CC-LR categories that are expected to have patterns of \n",
    "            interaction\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.rank: int\n",
    "            the expected rank after decomposition (= number of patterns in cell_lr_metadata)\n",
    "        '''\n",
    "        self.tensor_cci = tensor_cci\n",
    "        self.conditions = conditions\n",
    "        self.lr_labels = lr_labels\n",
    "        self.senders = senders\n",
    "        self.receivers = receivers\n",
    "        \n",
    "        self.lr_metadata = lr_metadata\n",
    "        self.cell_metadata = cell_metadata\n",
    "        self.cell_lr_metadata = cell_lr_metadata\n",
    "        self.rank = self.cell_lr_metadata.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_change_pattern(initial_value):\n",
    "    '''The maximum change in the average LR score given the starting value'''\n",
    "    decrease = False\n",
    "    if initial_value > 0.5:\n",
    "        initial_value = 0.5 - (initial_value - 0.5)\n",
    "        decrease = True\n",
    "    \n",
    "    if initial_value >= 0.2:\n",
    "        change = 2*initial_value\n",
    "    else:\n",
    "        change = initial_value + 0.2\n",
    "    \n",
    "    change = change - initial_value\n",
    "    \n",
    "    if decrease:\n",
    "        change = - change\n",
    "    \n",
    "    return change\n",
    "\n",
    "def linear(x, n_conditions):\n",
    "    return list(np.linspace(x[1], x[1] + x[0], n_conditions))\n",
    "\n",
    "def pulse(x, n_conditions):\n",
    "    change = x[0]\n",
    "    initial_val = x[1]\n",
    "    \n",
    "    vector = [initial_val] * n_conditions # initialize\n",
    "    \n",
    "    if n_conditions % 2 == 1:\n",
    "        mid_point = [math.floor(n_conditions/2)]\n",
    "    else:\n",
    "        mid_point = [n_conditions/2 - 1, n_conditions/2]\n",
    "\n",
    "    periph = None\n",
    "    if n_conditions >= 5: \n",
    "        periph = [min(mid_point)-1, max(mid_point)+1]\n",
    "    \n",
    "    for m in mid_point:\n",
    "        vector[int(m)] = initial_val + change\n",
    "    if periph is not None:\n",
    "        for p in periph:\n",
    "            vector[int(p)] = initial_val + (change*0.5)\n",
    "    return vector\n",
    "\n",
    "def oscillate(x, n_conditions):\n",
    "    osc_period = 3\n",
    "    if n_conditions > 3:\n",
    "        iter_vals = list(np.linspace(x[1], x[1] + x[0], osc_period))\n",
    "        iter_vals += [iter_vals[1]]#iter_vals[1:-1][::-1]\n",
    "\n",
    "        vector = list()\n",
    "        for i,j in enumerate(itertools.cycle(iter_vals)):\n",
    "            vector.append(j)\n",
    "            if i >= n_conditions - 1:\n",
    "                break\n",
    "        return vector\n",
    "    else:\n",
    "        return pulse(x, n_conditions)\n",
    "\n",
    "pattern_mapper = {'linear': linear, 'pulse': pulse, 'oscillate': oscillate}\n",
    "\n",
    "def generate_pattern(x, n_conditions):\n",
    "    vector = pattern_mapper[x[0]](x[1:], n_conditions)\n",
    "    vector[0] = x[2]\n",
    "    return vector\n",
    "\n",
    "class Simulate():\n",
    "    def __init__(self):\n",
    "        '''Initialize self\n",
    "\n",
    "        '''\n",
    "        self.cci = None\n",
    "    \n",
    "    def LR_network(self, network_type = None, B = None, subset = False, **params):\n",
    "        '''\n",
    "        Simulates a PPI network of *potential* ligand-receptor interactions, or extracts information. \\\n",
    "        from a use provided network.Defines one tensor dimension\n",
    "        Caveats: for a scale-free network, the number of ligands = the number of receptors \\\n",
    "                 for either network, there may be disconnected edges depending on \"p\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        network_type: str\n",
    "             \"scale-free\" to generate a scale-free network or \"normal\" to generate a network with a normal degree distribution\n",
    "        B: nx.Graph\n",
    "            a user provided undirected, unweighted bipartite network. Assumes in B.nodes, ligands are listed \\\n",
    "            before receptors. Takes precedence over network_type.\n",
    "        subset: bool\n",
    "            if B is provided and subset is true, this will take a random subset of the network, dropping disconnected nodes \\\n",
    "            (of a specified size, specfied in params)\n",
    "        **params: dict (keys for each option specified below)\n",
    "            the required parameters for generating a bipartite, undirected random network either scale-free or not. \\\n",
    "            \n",
    "            network_type = scale-free: keys - nodes, degrees, alpha, edges (see graphs.graph_generator.bipartite_sf for description) \n",
    "            network_type = normal: keys - n_ligands, n_receptors, p analogous to n,m,p in nx.bipartite.gnmk_random_graph\n",
    "            B != None: keys - n_ligands as described above\n",
    "            subset = True: keys - \n",
    "                n_ligands as described above\n",
    "                'subset_size' a value between (0,1) indicating the proportional \\\n",
    "                size of the subset (by nodes) compared to the network\n",
    "                'subset_type' either 'edges' or 'nodes' indicating whether to subset by removing nodes or edges \\\n",
    "                (edges recommended because they maintain the scale-free property)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self.LR: \n",
    "            populates LR object, key outputs outlined here\n",
    "        self.LR.B: nx.Graph\n",
    "            undirected bipartite graph with specified degree distribution (power or normal), or user specified B \\\n",
    "            disconnected nodes are removed\n",
    "        self.LR.edge_list: list\n",
    "            each entry is a tuple representing a potential interaction between a ligand-receptor pair, ligands on 0 index of each tuple\n",
    "\n",
    "\n",
    "        '''\n",
    "        gg = gg_() # return networkx object for graphs\n",
    "        user = False\n",
    "        if B is not None: #untested\n",
    "            user = True\n",
    "            # properties checked when calling gg.nx_to_edgelist\n",
    "            if network_type is not None:\n",
    "                warnings.warn('You have specified a network type and provided a network, B will take priority over network type')\n",
    "            if 'n_ligands' not in params:\n",
    "                raise ValueError('For a provided B, you must specify n_ligands in params')\n",
    "            \n",
    "            if subset:\n",
    "                if 'subset_size' not in params or 'subset_type' not in params:\n",
    "                    raise ValueError('To subset B, you must provide a desired subset_size and subset_type')\n",
    "                if params['subset_type'] == 'edges':\n",
    "                    B = gg.subset_edges(B, subset_size = params['subset_size'], drop = True)\n",
    "                elif params['subset_type'] == 'nodes': \n",
    "                    B = gg.subset_nodes(B, subset_size = params['subset_size'], drop = True)\n",
    "                else:\n",
    "                    raise ValueError(\"The subset_type param must be either 'edges' or 'nodes'\")\n",
    "            \n",
    "            \n",
    "        elif network_type == 'scale-free': \n",
    "            if 'degrees' not in params or 'nodes' not in params:\n",
    "                raise ValueError('Must specify degrees and nodes in **params')\n",
    "            if 'alpha' not in params: \n",
    "                params['alpha'] = 2 # also default in gg obj, didn't make it a **kwrag\n",
    "            if 'edges' not in params:\n",
    "                B, node_groups, fit, comp = gg.bipartite_sf(nodes = params['nodes'], degrees = params['degrees'], \n",
    "                                                            alpha = params['alpha'])\n",
    "            else:\n",
    "                B, node_groups, fit, comp = gg.bipartite_sf(nodes = params['nodes'], degrees = params['degrees'], \n",
    "                                                        alpha = params['alpha'], edges = ['edges'])  \n",
    "            B = B['nx']\n",
    "            params['n_ligands'] = params['nodes'] # same no. of ligands and receptors\n",
    "        elif network_type == 'normal':\n",
    "            if sorted(params) != ['n_ligands', 'n_receptors', 'p']:\n",
    "                raise ValueError('Must specify n_ligands, n_receptors in **params')\n",
    "            else:\n",
    "                B = nx.bipartite.random_graph(params['n_ligands'],params['n_receptors'], params['p'])\n",
    "        else:\n",
    "            raise ValueError('Must specify an appropriate network_type or provide a network B')\n",
    "        \n",
    "        B, edge_list, ng = gg.nx_to_edgelist(B, params['n_ligands']) # formatting/extract info\n",
    "        \n",
    "        # store PPI information in LR()\n",
    "        if user:\n",
    "            self.LR = LR(B, ng['1'], ng['2'], edge_list)\n",
    "        elif network_type == 'scale-free':\n",
    "            self.LR = LR(B, ng['1'], ng['2'], edge_list, network_type = network_type, \n",
    "                         alpha = params['alpha'], fit = fit, comp = comp)\n",
    "        elif network_type == 'normal':\n",
    "            self.LR = LR(B, ng['1'], ng['2'], edge_list, network_type = network_type, p = params['p'])\n",
    "\n",
    "    def emulate_sf_network(self, G):\n",
    "        '''Emulate a user-provided (recommended scale-free) network for L-R pair tensors dimension\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        G: nx.Graph\n",
    "            user-provided network (recommended scale-free)\n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        G2: nx.Graph\n",
    "            random bipartite scale-free network built using G's properties\n",
    "        \n",
    "        '''\n",
    "        gg = gg_()\n",
    "        fit = gg.power_fit(G)\n",
    "        if fit.p < 0.05:\n",
    "            warnings.warn('Input network is not scale-free')\n",
    "        print('----Simulated network------')\n",
    "        \n",
    "        G2, node_groups, fit2, comp = gg.bipartite_sf(nodes = round(len(G.nodes)), # should be 1/2 the nodes, but many are disconnected \n",
    "                                 degrees = np.median([i[1] for i in G.degree]),\n",
    "                                 alpha = fit.alpha, edges = len(G.edges),\n",
    "                                 check_properties = True, compare_barabasi = False)\n",
    "        G2 = G2['nx']\n",
    "        gg.drop_disconnected_nodes(G2)\n",
    "        return G2\n",
    "    \n",
    "    def generate_tensor_md(self, n_patterns, n_conditions, patterns = ['pulse', 'linear', 'oscillate'], \n",
    "                          consider_homotypic = True):\n",
    "        '''Generates cell-LR metadata pairs for tensor slices.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_patterns: int (> 0)\n",
    "            the number of CC - LR metadata pairs for which to form distinct interactions \n",
    "            the remaining backgroun will default to 0, with noise increasing this value\n",
    "            the groups with distinct interactions will each have distinct average values spaced b/w (0,1]\n",
    "        n_conditions: int (>2)\n",
    "            the number of conditions across which to generate tensor slices \n",
    "        patterns: list\n",
    "            list of strings, each of which should be included as a potential pattern for a given cell \n",
    "            metadata - LR metadata pair. Options: ['pulse', 'linear', 'exponential', 'oscillate']\n",
    "        consider_homotypic: bool\n",
    "            whether to allow homotypic interactions to have patterns. If False, homotypic interactions are guaranteed\n",
    "            to be a part of the background. \n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        self.clrm: pd.DataFrame\n",
    "            a list of metadata CC-LR pairs for which patterns of scores will change across conditions\n",
    "            alongside the expected average score for each condition\n",
    "        '''\n",
    "        #checks------------------------------------------------------------------------------------------------\n",
    "        allowed_patterns = ['pulse', 'linear', 'oscillate']\n",
    "        if patterns is not None:\n",
    "            if len(set(patterns).difference(allowed_patterns)) > 0:\n",
    "                raise ValueError('Patterns can only include: ' + ', '.join(allowed_patterns))\n",
    "        else:\n",
    "            patterns = allowed_patterns \n",
    "        \n",
    "        if n_conditions == 2 and set(['oscillate', 'pulse']).difference(patterns): \n",
    "            warnings.warn('At least 3 conditions are required for oscillations or pulses, only linear patterns will be considered')\n",
    "            patterns = ['linear']\n",
    "        if n_conditions <= 1:\n",
    "            warnings.warn('No conditions specified, only a tensor slice will be generated')\n",
    "            if n_conditions == 0:\n",
    "                n_conditions = 1\n",
    "        \n",
    "        self.n_conditions = n_conditions\n",
    "        \n",
    "       \n",
    "        #------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        n_lr_cat = len(self.LR.LR_metadata.subcategory.unique())\n",
    "        n_cc_cat = len(self.cci.cell_metadata.subcategory.unique())\n",
    "    \n",
    "        # all possible groups that have patterns of expression\n",
    "\n",
    "        lr_group = list()\n",
    "        for i in range(1, n_lr_cat + 3):\n",
    "            lr_group += list(itertools.combinations(self.LR.LR_metadata.subcategory.unique(), i))\n",
    "\n",
    "        # all possible cell groups that have patterns of expression\n",
    "        ccat_map = dict(zip(self.cci.cell_metadata.cell_id, self.cci.cell_metadata.subcategory))\n",
    "        ccati = list()\n",
    "        for ci in self.cci.cell_interactions:\n",
    "            ccati.append((ccat_map[ci[0]], ccat_map[ci[1]]))\n",
    "        ccati = pd.Series(ccati).unique().tolist()\n",
    "        if not consider_homotypic:\n",
    "            ccati = [cp for cp in ccati if cp[0] != cp[1]]\n",
    "\n",
    "        # all possible groups of cell metadata - LR metadata pairs\n",
    "        clrm = pd.DataFrame(columns = ['cell_subcat', 'LR_subcat'])\n",
    "        counter = 0\n",
    "        for i in list(itertools.product(ccati, lr_group)):\n",
    "            clrm.loc[counter, : ]= [i[0], i[1]]\n",
    "            counter += 1\n",
    "\n",
    "        # no all-all combinations\n",
    "        # clrm.drop(index = [clrm.shape[0] - 1], inplace = True)\n",
    "\n",
    "        if n_patterns > clrm.shape[0]:\n",
    "            warnings.warn('More patterns than possible specificed, setting to maximum possible: {}'.format(clrm.shape[0]))\n",
    "            n_patterns = clrm.shape[0]\n",
    "\n",
    "        # chose random subset to assign patterns to \n",
    "        clrm = clrm.loc[sorted(random.sample(clrm.index.tolist(), k = n_patterns)),]\n",
    "        clrm.reset_index(inplace = True, drop = True)\n",
    "        clrm.LR_subcat = clrm.LR_subcat.apply(lambda x: x[0])\n",
    "        \n",
    "        self.clrm = clrm\n",
    "        self.n_patterns = n_patterns\n",
    "        self.ts_frame = pd.DataFrame(columns = self.cci.cell_interactions, index = self.LR.edge_list)\n",
    "\n",
    "        # sort metadata categories\n",
    "        ccat_map = dict(zip(self.cci.cell_metadata.cell_id, self.cci.cell_metadata.subcategory))\n",
    "        LR_map = dict(zip(self.LR.LR_metadata.LR_id, self.LR.LR_metadata.subcategory))\n",
    "        self._lrcats = [LR_map[lri] for lri in self.ts_frame.index]\n",
    "        ccats = [(ccat_map[ci[0]], ccat_map[ci[1]]) for ci in self.ts_frame.columns]\n",
    "        \n",
    "        # get tensor slice coordinates for CC-LR pairs with expected patterns\n",
    "        # get tensor slice coordinates for CC-LR pairs with expected patterns\n",
    "        def get_coords(x):\n",
    "            '''Get the coords for each CC-LR metadata pair'''\n",
    "            col_coord = [j for j,cmd in enumerate(ccats) if cmd == x[0]]\n",
    "            row_coord = [i for i,lrmd in enumerate(self._lrcats) if lrmd == x[1]]\n",
    "            coords = list(itertools.product(row_coord, col_coord))\n",
    "            coords = [tuple(i[0] for i in coords), tuple([i[1] for i in coords])]\n",
    "            return coords\n",
    "        self.clrm['ts_coordinates'] = self.clrm[['cell_subcat', 'LR_subcat']].apply(lambda x: get_coords(x), axis = 1).tolist()\n",
    "        \n",
    "        # initial value\n",
    "        self.clrm[['0']] = list(np.arange(1/self.n_patterns, 1+1/self.n_patterns, 1/self.n_patterns))\n",
    "        \n",
    "        # patterns over time\n",
    "        ap = list()\n",
    "        for i in range(math.ceil(self.n_patterns/len(patterns))):\n",
    "            random.shuffle(patterns)\n",
    "            ap += patterns\n",
    "        self.clrm.insert(3, 'pattern', ap[:self.n_patterns])\n",
    "\n",
    "        self.clrm = pd.concat([self.clrm,\n",
    "                  pd.DataFrame(index = self.clrm.index, columns = [str(i) for i in range(1,self.n_conditions)])], axis = 1)\n",
    "        self.clrm.insert(3, 'change', self.clrm['0'].apply(lambda x: fold_change_pattern(x)))\n",
    "\n",
    "        # apply patterns to get averages across conditions\n",
    "        if self.n_conditions > 1:\n",
    "            self.clrm[[str(i) for i in range(self.n_conditions)]] = self.clrm[['pattern', 'change', '0']].apply(generate_pattern, args = (self.n_conditions,), axis = 1).tolist()\n",
    "\n",
    "    def generate_tensor(self, noise, binary = False, bulk = False):\n",
    "        '''Generates the tensor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        noise: float [0,1]\n",
    "            extent from which to perturb scores from the expected average value, including background\n",
    "        binary: bool\n",
    "            whether to have scores be continuous b/w [0,1] (False) or binary (True). Binary scoring not currently \n",
    "            implemented and must be set to False\n",
    "        bulk: bool\n",
    "            whether single-cells should be simulated (False) or cells should be grouped into metadata category (True)\n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        self.ts: dictionary\n",
    "            keys are labels for each condition (0 through n_conditions-1). Values are tensor slices with\n",
    "            columns as cell-cell pairs and rows as ligand-receptor pairs\n",
    "        '''\n",
    "        if not binary:\n",
    "            binary = False\n",
    "            warnings.warn('Only continuous scoring is currently implemented')\n",
    "        \n",
    "\n",
    "        self.ts = dict() # intitialize\n",
    "        c_labels = [str(i) for i in range(self.n_conditions)]\n",
    "        \n",
    "        if bulk: # consolidate cells into the metadata category\n",
    "            ccat_map = dict(zip(self.cci.cell_metadata.cell_id, self.cci.cell_metadata.subcategory))\n",
    "            ccat_map = {cpi: (ccat_map[cpi[0]], ccat_map[cpi[1]]) for cpi in self.ts_frame.columns}\n",
    "\n",
    "            # change matrix to just include cell metadata groupings rather than individual cell IDs\n",
    "            self.ts_frame = pd.DataFrame(index = self.ts_frame.index, columns = sorted(set(ccat_map.values())))\n",
    "\n",
    "\n",
    "            # rewrite coordinates according to bulk groupings\n",
    "            def get_coords(x):\n",
    "                '''Get the coords for each CC-LR metadata pair'''\n",
    "                col_coord = [j for j,cmd in enumerate(self.ts_frame.columns.tolist()) if cmd == x[0]]\n",
    "                row_coord = [i for i,lrmd in enumerate(self._lrcats) if lrmd == x[1]]\n",
    "                coords = list(itertools.product(row_coord, col_coord))\n",
    "                coords = [tuple(i[0] for i in coords), tuple([i[1] for i in coords])]\n",
    "                return coords\n",
    "            self.clrm['ts_coordinates'] = self.clrm[['cell_subcat', 'LR_subcat']].apply(lambda x: get_coords(x), axis = 1).tolist()\n",
    "\n",
    "        if noise == 0:\n",
    "            for cond in c_labels:\n",
    "                df = self.ts_frame.copy()\n",
    "                for idx in self.clrm.index:\n",
    "                    avg_val = self.clrm.loc[idx, cond]\n",
    "                    coords = self.clrm.loc[idx, 'ts_coordinates']\n",
    "                    df.values[coords] = avg_val # non-backgroun \n",
    "                df.fillna(0, inplace = True) # background\n",
    "                self.ts[cond] = df\n",
    "        else:\n",
    "            min_val = self.clrm[[str(i) for i in range(self.n_conditions)]].min().min()\n",
    "            scale = min_val/np.array([utils.piecewise_fit(min_val, *utils.fit_params)])[0]\n",
    "            for cond in c_labels:\n",
    "                df = self.ts_frame.copy()\n",
    "                # background\n",
    "                vals = utils.get_truncated_normal(n = self.ts_frame.shape[0]*self.ts_frame.shape[1], \n",
    "                                                  sd = noise*min_val, mean = 0)*scale\n",
    "                df[:] = vals.reshape(self.ts_frame.shape)\n",
    "                for idx in self.clrm.index: # non-background\n",
    "                    avg_val = self.clrm.loc[idx, cond]\n",
    "                    coords = self.clrm.loc[idx, 'ts_coordinates']\n",
    "                    df.values[coords] = utils.get_truncated_normal(n = len(coords[0]), \n",
    "                                                                   sd = noise*avg_val, mean = avg_val)\n",
    "                self.ts[cond] = df\n",
    "\n",
    "    def reshape(self):\n",
    "        '''Creates tensor in the format necessary for running with tensor-cell2cell and stores \n",
    "        as a sim_tensor object under self.sim_tensor\n",
    "        \n",
    "        Formatting:\n",
    "        - communication matrix: a matrix of sender by receiver cells for a given LR pair\n",
    "        - 3D tensor: each slice is a communication matrix, the full tensor is all slices for all LR pairs\n",
    "        - final tensor: each slice is the 3D tensor, the full tensor is all slices for all conditions\n",
    "        '''\n",
    "        senders = sorted(set([i[0] for i in self.ts_frame.columns]))\n",
    "        receivers = sorted(set([i[1] for i in self.ts_frame.columns]))\n",
    "\n",
    "\n",
    "        # map CC coordinates in CC-LR matrix to .iloc coordinates for fast filling of \"communication\" matrix\n",
    "        tcs_coords = dict()\n",
    "        for coord_i, cell_i in enumerate(senders):\n",
    "            for coord_j, cell_j in enumerate(receivers):\n",
    "                tcs_coords[(cell_i,cell_j)] = (coord_i, coord_j)\n",
    "\n",
    "        coords = [tcs_coords[col] for col in self.ts_frame.columns]\n",
    "        coords = [tuple([i[0] for i in coords]), tuple([i[1] for i in coords])]\n",
    "\n",
    "        tcs_ = np.full([len(senders),len(receivers)], np.nan)\n",
    "\n",
    "        tensor_list = list()\n",
    "        print('Generate reshaped tensor')\n",
    "        for cond in tqdm(self.ts):\n",
    "            tcs_list = list()\n",
    "            ts = self.ts_frame.copy()\n",
    "            ts[:] = self.ts[cond].values # assign values for specific condition in a CC-LR matrix\n",
    "            for idx in range(self.ts_frame.shape[0]):\n",
    "                tcs = tcs_.copy() # generate a \"communication\" matrix and fill with each row of the CC-LR matrix\n",
    "                tcs[coords] = ts.iloc[idx,:].values\n",
    "                tcs_list.append(tcs)\n",
    "            tensor_list.append(tensorly.tensor(tcs_list))\n",
    "        tensor_cci = tensorly.tensor(tensor_list)\n",
    "        del tensor_list\n",
    "\n",
    "        # store in a sim_tensor object\n",
    "        self.sim_tensor = sim_tensor(tensor_cci, conditions = list(self.ts.keys()), \n",
    "                                     lr_labels = self.ts_frame.index.tolist(), senders = senders, receivers = receivers, \n",
    "                                    lr_metadata = self.LR.LR_metadata, cell_metadata = self.cci.cell_metadata, \n",
    "                                     cell_lr_metadata = self.clrm.drop(columns=['ts_coordinates']))\n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hratch/Projects/cci_dt/notebooks/simulation/tmpw3zm8bxd_bipartite_sf.csv\n",
      "Generate undirected, bipartite, scale-free graph\n",
      "Check network properties\n",
      "All properties are as expected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../scripts/simulation/graphs.py:164: UserWarning: 67 nodes are disconnected, removing from network\n",
      "  warnings.warn(mssg)\n",
      "/home/hratch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:340: UserWarning: Only continuous scoring is currently implemented\n",
      "/home/hratch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:370: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]/home/hratch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:421: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      " 17%|█▋        | 2/12 [00:00<00:00, 19.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate reshaped tensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 19.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "sim = Simulate() \n",
    "# sim_norm = Simulate()\n",
    "\n",
    "# simulate a scale_free randomly connected ligand-receptor network (potential interactions)\n",
    "sim.LR_network(network_type = 'scale-free', **{'nodes': 100, 'degrees': 3, 'alpha': 2}) #scale-free\n",
    "\n",
    "# # simulate a ranodmly connected network with nomral distributions\n",
    "# sim_norm.LR_network(network_type = 'normal', **{'n_ligands': 500, 'n_receptors': 500, 'p': 0.5}) # normally distributed\n",
    "\n",
    "\n",
    "# LR metadata\n",
    "sim.LR.generate_metadata(n_LR_cats = {3: 0}, cat_skew = 0)\n",
    "\n",
    "# generate cell metadata, accounting for directionality (senders vs receivers) and \n",
    "# allowing for autocrine interactions \n",
    "cci = CCI_MD()\n",
    "cci.cci_network(n_cells = 50, directional = True, autocrine = True)\n",
    "\n",
    "# generate 1 metadata categories, with 3 subcategories and 0 skew, the overall skew of categories is 0\n",
    "# do not remove homotypic interactions (will be included)\n",
    "cci.generate_metadata(n_cell_cats = {3: 0}, cat_skew = 0, remove_homotypic = 0)\n",
    "# add cell metadata to simulation object\n",
    "sim.cci = cci\n",
    "\n",
    "# generate n_patter metadata groups of CC-LR pairs that change across n_conditions\n",
    "# these changes can either be linear, oscillating, or a pulse; allow homotypic interactions to form patterns\n",
    "sim.generate_tensor_md(n_patterns = 4, n_conditions = 12, patterns = ['pulse', 'linear', 'oscillate'], \n",
    "                      consider_homotypic = True)\n",
    "\n",
    "#generate a tensor with continuous LR scores and no noise\n",
    "sim.generate_tensor(noise = 0, binary = False, bulk = True)\n",
    "\n",
    "# format the tensor to be input to tensor-cell2cell\n",
    "sim.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# if not os.path.isdir('tmp_sim_forerick'):\n",
    "#     os.mkdir('tmp_sim_forerick')\n",
    "# for k,v in sim.sim_tensor.__dict__.items():\n",
    "#     if k != 'rank':\n",
    "#         with open('tmp_sim_forerick/' + k + '.pickle','wb') as f:\n",
    "#             pickle.dump(v, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cci_dt] *",
   "language": "python",
   "name": "conda-env-cci_dt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
