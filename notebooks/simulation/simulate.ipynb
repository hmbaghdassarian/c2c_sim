{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hratch/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../scripts/')\n",
    "from simulation.graphs import graph_generator as gg_\n",
    "from simulation import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_bias(n, skew):\n",
    "    '''\n",
    "    n: int\n",
    "        Number of entries to bin\n",
    "    skew: extent to which skew binning\n",
    "    '''\n",
    "    unbiased = [1/n]*n\n",
    "    if skew == 0:\n",
    "        biased = np.array(unbiased)\n",
    "    elif skew == 1:\n",
    "        biased = [0]*len(unbiased)\n",
    "        biased[-1] = 1\n",
    "        biased = np.array(biased)\n",
    "    else:\n",
    "        X = np.linspace(0, len(unbiased), len(unbiased))\n",
    "        biased = skewnorm.pdf(X, a = 1, loc = len(unbiased), scale = (1-skew/1)*len(unbiased))\n",
    "        biased = biased/sum(biased)\n",
    "    return biased*100\n",
    "\n",
    "class LR():\n",
    "    '''object to store metadata and relevant information for the ligan-receptor dimension of tensor\n",
    "    for internal use\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, B, ligands, receptors, edge_list, network_type = None, alpha = None, \n",
    "                 fit = None, comp = None, p = None):\n",
    "        '''Initialize\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        B: nx.Graph\n",
    "            a undirected bipartite network representing PPI between ligands and receptors (direction would always be L-->R)\n",
    "        ligands: list \n",
    "            ligand IDs for each protein\n",
    "        receptors: list\n",
    "            receptor IDs for each protein\n",
    "        self.edge_list: list\n",
    "            each entry is a tuple representing a potential interaction between a ligand-receptor pair, ligands on 0 index of each tuple\n",
    "        network_type: str\n",
    "            \"scale-free\" indicates scale-free network, \"normal\" indicates a normal degree distribution\n",
    "        alpha: float\n",
    "            scale-free exponent for network degree distribution (recommended 2<alpha<3)\n",
    "        p: float\n",
    "            probability of adding an edge when using network_type option = 'normal'\n",
    "        fit: igraph.FittedPowerLaw\n",
    "            scale-free network parameters for B_ig (p-value from Kolmogrov-Smirnov test)\n",
    "        comp: pd.DataFrame or None\n",
    "            summary of differences in network properties between  bipartite network and similar Barabasi network\n",
    "        '''\n",
    "        if network_type is None:\n",
    "            self.network_type = 'user-speficied'\n",
    "        else:\n",
    "            self.network_type = network_type\n",
    "        self.B = B\n",
    "        self.ligands = ligands\n",
    "        self.receptors = receptors\n",
    "        self.edge_list = edge_list\n",
    "        self.alpha = alpha\n",
    "        self.fit = fit\n",
    "        self.comp = comp\n",
    "        self.p = p\n",
    "    def generate_metadata(self, n_LR_cats = {2: 0}, cat_skew = 0):\n",
    "        '''Generate metadata groupings for the L-R pairs. Categories are defined as distinct types of \n",
    "        metadata associated with the LR pair, e.g. \"signaling pathway\". Subcategories are\n",
    "        the associated labels within a category, e.g. \"growth\" and \"inflammation\" within the \"signaling pathway\" category.\n",
    "\n",
    "        Note: For skew, 0 means evenly distributed, 1 means all LR pairs fall into the first category/subcategory. \n",
    "\n",
    "        n_LR_cats: dict\n",
    "            The length of the dictionary represents the total number of categories associated with the LR\n",
    "            Each key is an integer representing the number of subcategories for the particular category. \n",
    "            Each value is a float [0,1] indicating the skew of distribution of LRs across \n",
    "            subcategories within each category. \n",
    "        cat_skew: float [0,1]\n",
    "            Skew of distribution of LRs across categories\n",
    "\n",
    "        '''\n",
    "        if len(n_LR_cats) > 1:\n",
    "            raise ValueError('Currently, only one metadata category can be considered')\n",
    "        # group each LR into the categories above\n",
    "        # generate categories\n",
    "        LR_categories = [str(uuid.uuid4()).split('-')[-1] for i in range(len(n_LR_cats))]\n",
    "        cat_bias = weight_bias(n = len(LR_categories), skew = 0)\n",
    "        self.LR_metadata = pd.DataFrame(data = {'LR_id': self.edge_list, \n",
    "                        'category': random.choices(population = LR_categories, weights=cat_bias, \n",
    "                                                  k=len(self.edge_list))})\n",
    "\n",
    "        # generate subcategories\n",
    "        self.LR_metadata['subcategory'] = float('nan')\n",
    "        i = 0\n",
    "        for n_subcat, subcat_skew in n_LR_cats.items():\n",
    "            sub = self.LR_metadata[self.LR_metadata.category == LR_categories[i]]\n",
    "            subcat_bias = weight_bias(n = n_subcat, skew = subcat_skew)\n",
    "            self.LR_metadata.loc[sub.index, 'subcategory'] = random.choices(population = [str(uuid.uuid4()).split('-')[-1] for i in range(n_subcat)], \n",
    "                           weights=subcat_bias, k=sub.shape[0])\n",
    "            i += 1\n",
    "\n",
    "class CCI_MD():\n",
    "    '''Generate the CCI network for the tensor slice at time point 0'''\n",
    "    \n",
    "    def cci_network(self, n_cells, directional = True):\n",
    "            '''Initialize the cell-cell interaction network.\n",
    "\n",
    "            n_cells: int\n",
    "                the total number of cells to simulate; all cell-cell pairs will have a potential interaction, but only\n",
    "                those that actually interact will have a score > 0 in the tensor slice\n",
    "            directional: bool\n",
    "                whether cell-cell interactions are directional (tuple of cell (A,B) indicates interaction from A-->B) or \n",
    "                not\n",
    "\n",
    "            '''\n",
    "            # generate random cell ids\n",
    "            self.cell_ids = [str(uuid.uuid4()).split('-')[-1] for i in range(n_cells)]\n",
    "            if directional:\n",
    "                self.cell_interactions = list(itertools.permutations(self.cell_ids, 2))\n",
    "            else:\n",
    "                self.cell_interactions = list(itertools.combinations(self.cell_ids, 2))\n",
    "    def generate_metadata(self, n_cell_cats = {2: 0}, cat_skew = 0, \n",
    "                         remove_homotypic = None):\n",
    "        '''Generate metadata groupings for the cells (individual). Categories are defined as distinct types of \n",
    "        metadata associated with the cell or protein, e.g. \"cell type\" and \"cell cycle phase\". Subcategories are\n",
    "        the associated labels within a category, e.g. \"T-cell\" and \"dendritic cell\" within the \"cell type\" category.\n",
    "        \n",
    "        Note: For skew, 0 means evenly distributed, 1 means all cells fall into the first category/subcategory. \n",
    "        \n",
    "        n_cell_cats: dict\n",
    "            The length of the dictionary represents the total number of categories associated with the cell\n",
    "            Each key is an integer representing the number of subcategories for the particular category. \n",
    "            Each value is a float [0,1] indicating the skew of distribution of cells across \n",
    "            subcategories within each category. \n",
    "        cat_skew: float [0,1]\n",
    "            Skew of distribution of cells across categories\n",
    "        remove_homotypic: int\n",
    "            whether to remove homotypic ineractions between cells by cell category; how many categories to consider; \n",
    "            must be <= the number of categories present in the metadata\n",
    "\n",
    "        '''\n",
    "        if len(n_cell_cats) > 1:\n",
    "            raise ValueError('Currently, only one metadata category can be considered')\n",
    "        if remove_homotypic > len(n_cell_cats):\n",
    "            raise ValueError('The value for \"remove_homotypic\" cannot be larger than the total number of categories associated with the cells')\n",
    "\n",
    "        # group each cell into the categories above\n",
    "        # generate categories\n",
    "        cell_categories = [str(uuid.uuid4()).split('-')[-1] for i in range(len(n_cell_cats))]\n",
    "        cat_bias = weight_bias(n = len(cell_categories), skew = 0)\n",
    "        self.cell_ids = pd.DataFrame(data = {'cell_id': self.cell_ids, \n",
    "                        'category': random.choices(population = cell_categories, weights=cat_bias, \n",
    "                                                  k=len(self.cell_ids))})\n",
    "        \n",
    "        # generate subcategories\n",
    "        self.cell_ids['subcategory'] = float('nan')\n",
    "        i = 0\n",
    "        for n_subcat, subcat_skew in n_cell_cats.items():\n",
    "            sub = self.cell_ids[self.cell_ids.category == cell_categories[i]]\n",
    "            subcat_bias = weight_bias(n = n_subcat, skew = subcat_skew)\n",
    "            self.cell_ids.loc[sub.index, 'subcategory'] = random.choices(population = [str(uuid.uuid4()).split('-')[-1] for i in range(n_subcat)], \n",
    "                           weights=subcat_bias, k=sub.shape[0])\n",
    "            i += 1\n",
    "        self.cell_metadata = self.cell_ids\n",
    "        del self.cell_ids\n",
    "        \n",
    "\n",
    "        if remove_homotypic is not None and remove_homotypic > 0: # remove homotypic interactions of a given category\n",
    "            print('Remove homotypic cell interactions for {} categories'.format(remove_homotypic))\n",
    "            i = 0\n",
    "            to_remove = list()\n",
    "            while i < remove_homotypic:\n",
    "                cat = cell_categories[i]\n",
    "                sub = self.cell_metadata[self.cell_metadata.category == cell_categories[i]]\n",
    "                cell_ids = sub.cell_id.tolist()\n",
    "\n",
    "                for ccp in self.cell_interactions:\n",
    "                    sub_ = sub[(sub.cell_id == ccp[0]) | (sub.cell_id == ccp[1])]\n",
    "                    if sub_.shape[0] == 2 and sub_.subcategory.unique().shape[0] == 1:\n",
    "                        to_remove.append(ccp)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            # remove homotypic interactions as identified above for categories 1-i\n",
    "            self.cell_interactions = list(set(self.cell_interactions).difference(to_remove))\n",
    "\n",
    "            # filter out any cells that no longer are present \n",
    "            cell_ids = list(set(sum(list(zip(*self.cell_interactions)), ())))\n",
    "            self.cell_metadata = self.cell_metadata[self.cell_metadata.cell_id.isin(cell_ids)]\n",
    "            self.cell_metadata.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_change_pattern(initial_value):\n",
    "    '''The maximum change in the average LR score given the starting value'''\n",
    "    decrease = False\n",
    "    if initial_value > 0.5:\n",
    "        initial_value = 0.5 - (initial_value - 0.5)\n",
    "        decrease = True\n",
    "    \n",
    "    if initial_value >= 0.2:\n",
    "        change = 2*initial_value\n",
    "    else:\n",
    "        change = initial_value + 0.2\n",
    "    \n",
    "    change = change - initial_value\n",
    "    \n",
    "    if decrease:\n",
    "        change = - change\n",
    "    \n",
    "    return change\n",
    "\n",
    "def linear(x, n_conditions):\n",
    "    return list(np.linspace(x[1], x[1] + x[0], n_conditions))\n",
    "\n",
    "def pulse(x, n_conditions):\n",
    "    change = x[0]\n",
    "    initial_val = x[1]\n",
    "    \n",
    "    vector = [initial_val] * n_conditions # initialize\n",
    "    \n",
    "    if n_conditions % 2 == 1:\n",
    "        mid_point = [math.floor(n_conditions/2)]\n",
    "    else:\n",
    "        mid_point = [n_conditions/2 - 1, n_conditions/2]\n",
    "\n",
    "    periph = None\n",
    "    if n_conditions >= 5: \n",
    "        periph = [min(mid_point)-1, max(mid_point)+1]\n",
    "    \n",
    "    for m in mid_point:\n",
    "        vector[int(m)] = initial_val + change\n",
    "    if periph is not None:\n",
    "        for p in periph:\n",
    "            vector[int(p)] = initial_val + (change*0.5)\n",
    "    return vector\n",
    "\n",
    "def oscillate(x, n_conditions):\n",
    "    osc_period = 3\n",
    "    if n_conditions > 3:\n",
    "        iter_vals = list(np.linspace(x[1], x[1] + x[0], osc_period))\n",
    "        iter_vals += [iter_vals[1]]#iter_vals[1:-1][::-1]\n",
    "\n",
    "        vector = list()\n",
    "        for i,j in enumerate(itertools.cycle(iter_vals)):\n",
    "            vector.append(j)\n",
    "            if i >= n_conditions - 1:\n",
    "                break\n",
    "        return vector\n",
    "    else:\n",
    "        return pulse(x, n_conditions)\n",
    "\n",
    "pattern_mapper = {'linear': linear, 'pulse': pulse, 'oscillate': oscillate}\n",
    "\n",
    "def generate_pattern(x, n_conditions):\n",
    "    pattern = x[0]\n",
    "    return pattern_mapper[pattern](x[1:], n_conditions)\n",
    "\n",
    "class Simulate():\n",
    "    def __init__(self):\n",
    "        '''Initialize self\n",
    "\n",
    "        '''\n",
    "        self.cci = None\n",
    "    \n",
    "    def LR_network(self, network_type = None, B = None, subset = False, **params):\n",
    "        '''\n",
    "        Simulates a PPI network of *potential* ligand-receptor interactions, or extracts information. \\\n",
    "        from a use provided network.Defines one tensor dimension\n",
    "        Caveats: for a scale-free network, the number of ligands = the number of receptors \\\n",
    "                 for either network, there may be disconnected edges depending on \"p\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        network_type: str\n",
    "             \"scale-free\" to generate a scale-free network or \"normal\" to generate a network with a normal degree distribution\n",
    "        B: nx.Graph\n",
    "            a user provided undirected, unweighted bipartite network. Assumes in B.nodes, ligands are listed \\\n",
    "            before receptors. Takes precedence over network_type.\n",
    "        subset: bool\n",
    "            if B is provided and subset is true, this will take a random subset of the network, dropping disconnected nodes \\\n",
    "            (of a specified size, specfied in params)\n",
    "        **params: dict (keys for each option specified below)\n",
    "            the required parameters for generating a bipartite, undirected random network either scale-free or not. \\\n",
    "            \n",
    "            network_type = scale-free: keys - nodes, degrees, alpha, edges (see graphs.graph_generator.bipartite_sf for description) \n",
    "            network_type = normal: keys - n_ligands, n_receptors, p analogous to n,m,p in nx.bipartite.gnmk_random_graph\n",
    "            B != None: keys - n_ligands as described above\n",
    "            subset = True: keys - \n",
    "                n_ligands as described above\n",
    "                'subset_size' a value between (0,1) indicating the proportional \\\n",
    "                size of the subset (by nodes) compared to the network\n",
    "                'subset_type' either 'edges' or 'nodes' indicating whether to subset by removing nodes or edges \\\n",
    "                (edges recommended because they maintain the scale-free property)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self.LR: \n",
    "            populates LR object, key outputs outlined here\n",
    "        self.LR.B: nx.Graph\n",
    "            undirected bipartite graph with specified degree distribution (power or normal), or user specified B \\\n",
    "            disconnected nodes are removed\n",
    "        self.LR.edge_list: list\n",
    "            each entry is a tuple representing a potential interaction between a ligand-receptor pair, ligands on 0 index of each tuple\n",
    "\n",
    "\n",
    "        '''\n",
    "        gg = gg_() # return networkx object for graphs\n",
    "        user = False\n",
    "        if B is not None: #untested\n",
    "            user = True\n",
    "            # properties checked when calling gg.nx_to_edgelist\n",
    "            if network_type is not None:\n",
    "                warnings.warn('You have specified a network type and provided a network, B will take priority over network type')\n",
    "            if 'n_ligands' not in params:\n",
    "                raise ValueError('For a provided B, you must specify n_ligands in params')\n",
    "            \n",
    "            if subset:\n",
    "                if 'subset_size' not in params or 'subset_type' not in params:\n",
    "                    raise ValueError('To subset B, you must provide a desired subset_size and subset_type')\n",
    "                if params['subset_type'] == 'edges':\n",
    "                    B = gg.subset_edges(B, subset_size = params['subset_size'], drop = True)\n",
    "                elif params['subset_type'] == 'nodes': \n",
    "                    B = gg.subset_nodes(B, subset_size = params['subset_size'], drop = True)\n",
    "                else:\n",
    "                    raise ValueError(\"The subset_type param must be either 'edges' or 'nodes'\")\n",
    "            \n",
    "            \n",
    "        elif network_type == 'scale-free': \n",
    "            if 'degrees' not in params or 'nodes' not in params:\n",
    "                raise ValueError('Must specify degrees and nodes in **params')\n",
    "            if 'alpha' not in params: \n",
    "                params['alpha'] = 2 # also default in gg obj, didn't make it a **kwrag\n",
    "            if 'edges' not in params:\n",
    "                B, node_groups, fit, comp = gg.bipartite_sf(nodes = params['nodes'], degrees = params['degrees'], \n",
    "                                                            alpha = params['alpha'])\n",
    "            else:\n",
    "                B, node_groups, fit, comp = gg.bipartite_sf(nodes = params['nodes'], degrees = params['degrees'], \n",
    "                                                        alpha = params['alpha'], edges = ['edges'])  \n",
    "            B = B['nx']\n",
    "            params['n_ligands'] = params['nodes'] # same no. of ligands and receptors\n",
    "        elif network_type == 'normal':\n",
    "            if sorted(params) != ['n_ligands', 'n_receptors', 'p']:\n",
    "                raise ValueError('Must specify n_ligands, n_receptors in **params')\n",
    "            else:\n",
    "                B = nx.bipartite.random_graph(params['n_ligands'],params['n_receptors'], params['p'])\n",
    "        else:\n",
    "            raise ValueError('Must specify an appropriate network_type or provide a network B')\n",
    "        \n",
    "        B, edge_list, ng = gg.nx_to_edgelist(B, params['n_ligands']) # formatting/extract info\n",
    "        \n",
    "        # store PPI information in LR()\n",
    "        if user:\n",
    "            self.LR = LR(B, ng['1'], ng['2'], edge_list)\n",
    "        elif network_type == 'scale-free':\n",
    "            self.LR = LR(B, ng['1'], ng['2'], edge_list, network_type = network_type, \n",
    "                         alpha = params['alpha'], fit = fit, comp = comp)\n",
    "        elif network_type == 'normal':\n",
    "            self.LR = LR(B, ng['1'], ng['2'], edge_list, network_type = network_type, p = params['p'])\n",
    "\n",
    "    def emulate_sf_network(self, G):\n",
    "        '''Emulate a user-provided (recommended scale-free) network for L-R pair tensors dimension\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        G: nx.Graph\n",
    "            user-provided network (recommended scale-free)\n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        G2: nx.Graph\n",
    "            random bipartite scale-free network built using G's properties\n",
    "        \n",
    "        '''\n",
    "        gg = gg_()\n",
    "        fit = gg.power_fit(G)\n",
    "        if fit.p < 0.05:\n",
    "            warnings.warn('Input network is not scale-free')\n",
    "        print('----Simulated network------')\n",
    "        \n",
    "        G2, node_groups, fit2, comp = gg.bipartite_sf(nodes = round(len(G.nodes)), # should be 1/2 the nodes, but many are disconnected \n",
    "                                 degrees = np.median([i[1] for i in G.degree]),\n",
    "                                 alpha = fit.alpha, edges = len(G.edges),\n",
    "                                 check_properties = True, compare_barabasi = False)\n",
    "        G2 = G2['nx']\n",
    "        gg.drop_disconnected_nodes(G2)\n",
    "        return G2\n",
    "    \n",
    "    def generate_tensor_md(self, n_patterns, n_conditions, patterns = ['pulse', 'linear', 'oscillate']):\n",
    "        '''Generates cell-LR metadata pairs for tensor slices.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_patterns: int (> 0)\n",
    "            the number of CC - LR metadata pairs for which to form distinct interactions \n",
    "            the remaining backgroun will default to 0, with noise increasing this value\n",
    "            the groups with distinct interactions will each have distinct average values spaced b/w (0,1]\n",
    "        n_conditions: int (>2)\n",
    "            the number of conditions across which to generate tensor slices \n",
    "        patterns: list\n",
    "            list of strings, each of which should be included as a potential pattern for a given cell \n",
    "            metadata - LR metadata pair. Options: ['pulse', 'linear', 'exponential', 'oscillate']\n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        self.clrm: pd.DataFrame\n",
    "            a list of metadata CC-LR pairs for which patterns of scores will change across conditions\n",
    "            alongside the expected average score for each condition\n",
    "        '''\n",
    "        #checks------------------------------------------------------------------------------------------------\n",
    "        if n_conditions <=2:\n",
    "            warnings.warn('At least 4 conditions are required')\n",
    "            n_conditions = 3\n",
    "        self.n_conditions = n_conditions\n",
    "        \n",
    "        allowed_patterns = ['pulse', 'linear', 'oscillate']\n",
    "        if patterns is not None:\n",
    "            if len(set(patterns).difference(allowed_patterns)) > 0:\n",
    "                raise ValueError('Patterns can only include: ' + ', '.join(allowed_patterns))\n",
    "        else:\n",
    "            patterns = allowed_patterns        \n",
    "        #------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        n_lr_cat = len(self.LR.LR_metadata.subcategory.unique())\n",
    "        n_cc_cat = len(self.LR.LR_metadata.subcategory.unique())\n",
    "    \n",
    "        # all possible groups that have patterns of expression\n",
    "\n",
    "        lr_group = list()\n",
    "        for i in range(1, n_lr_cat + 3):\n",
    "            lr_group += list(itertools.combinations(self.LR.LR_metadata.subcategory.unique(), i))\n",
    "\n",
    "        # all possible cell groups that have patterns of expression\n",
    "        ccat_map = dict(zip(self.cci.cell_metadata.cell_id, self.cci.cell_metadata.subcategory))\n",
    "        ccati = list()\n",
    "        for ci in self.cci.cell_interactions:\n",
    "            ccati.append((ccat_map[ci[0]], ccat_map[ci[1]]))\n",
    "        ccati = pd.Series(ccati).unique()\n",
    "\n",
    "        # all possible groups of cell metadata - LR metadata pairs\n",
    "        clrm = pd.DataFrame(columns = ['cell_subcat', 'LR_subcat'])\n",
    "        counter = 0\n",
    "        for i in list(itertools.product(ccati, lr_group)):\n",
    "            clrm.loc[counter, : ]= [i[0], i[1]]\n",
    "            counter += 1\n",
    "\n",
    "        # no all-all combinations\n",
    "        # clrm.drop(index = [clrm.shape[0] - 1], inplace = True)\n",
    "\n",
    "        if n_patterns > clrm.shape[0]:\n",
    "            warnings.warn('More patterns than possible specificed, setting to maximum possible: {}'.format(clrm.shape[0]))\n",
    "            n_patterns = clrm.shape[0]\n",
    "\n",
    "        for i in range(n_patterns):\n",
    "            clrm = clrm.loc[sorted(random.sample(clrm.index.tolist(), k = n_patterns)),]\n",
    "        clrm.reset_index(inplace = True, drop = True)\n",
    "        clrm.LR_subcat = clrm.LR_subcat.apply(lambda x: x[0])\n",
    "        \n",
    "        self.clrm = clrm\n",
    "        self.n_patterns = n_patterns\n",
    "        self.ts_frame = pd.DataFrame(columns = self.cci.cell_interactions, index = self.LR.edge_list)\n",
    "\n",
    "        # sort metadata categories\n",
    "        ccat_map = dict(zip(self.cci.cell_metadata.cell_id, self.cci.cell_metadata.subcategory))\n",
    "        LR_map = dict(zip(self.LR.LR_metadata.LR_id, self.LR.LR_metadata.subcategory))\n",
    "        lrcats = [LR_map[lri] for lri in self.ts_frame.index]\n",
    "        ccats = [(ccat_map[ci[0]], ccat_map[ci[1]]) for ci in self.ts_frame.columns]\n",
    "        \n",
    "        # get tensor slice coordinates for CC-LR pairs with expected patterns\n",
    "        def get_coords(i):\n",
    "            coords = list(zip([k for k in range(len(ccats)) if \\\n",
    "                                         ccats[k] == self.clrm.loc[i, 'cell_subcat']], \n",
    "                                         [k for k in range(len(lrcats)) if lrcats[k] == self.clrm.loc[i, 'LR_subcat']]))\n",
    "            return [tuple([i[0] for i in coords]), tuple([i[1] for i in coords])]\n",
    "        self.clrm['ts_coordinates'] = pd.Series(self.clrm.index).apply(lambda i: get_coords(i)).tolist()\n",
    "        \n",
    "        # initial value\n",
    "        self.clrm[['0']] = list(np.arange(1/self.n_patterns, 1+1/self.n_patterns, 1/self.n_patterns))\n",
    "        \n",
    "        # patterns over time\n",
    "        ap = list()\n",
    "        for i in range(math.ceil(self.n_patterns/len(patterns))):\n",
    "            random.shuffle(patterns)\n",
    "            ap += patterns\n",
    "        self.clrm.insert(3, 'pattern', ap[:self.n_patterns])\n",
    "\n",
    "        self.clrm = pd.concat([self.clrm,\n",
    "                  pd.DataFrame(index = self.clrm.index, columns = [str(i) for i in range(1,self.n_conditions)])], axis = 1)\n",
    "        self.clrm.insert(3, 'change', self.clrm['0'].apply(lambda x: fold_change_pattern(x)))\n",
    "\n",
    "        # apply patterns to get averages across conditions\n",
    "        self.clrm[[str(i) for i in range(self.n_conditions)]] = self.clrm[['pattern', 'change', '0']].apply(generate_pattern, args = (self.n_conditions,), axis = 1).tolist()\n",
    "\n",
    "    def generate_tensor(self, noise, binary = False):\n",
    "        '''Generates the tensor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        noise: float [0,1]\n",
    "            extent from which to perturb scores from the expected average value, including background\n",
    "        binary: bool\n",
    "            whether to have scores be continuous b/w [0,1] (False) or binary (True). Binary scoring not currently \n",
    "            implemented and must be set to False\n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        self.ts: dictionary\n",
    "            keys are labels for each condition (0 through n_conditions-1). Values are tensor slices with\n",
    "            columns as cell-cell pairs and rows as ligand-receptor pairs\n",
    "        '''\n",
    "        if not binary:\n",
    "            binary = False\n",
    "            warnings.warn('Only continuous scoring is currently implemented')\n",
    "        # initialize tensor slices\n",
    "        self.ts = {str(i): self.ts_frame.copy() for i in range(self.n_conditions)}\n",
    "\n",
    "        # generate the background\n",
    "        print('Generate background noise')\n",
    "        if noise == 0:\n",
    "            for i in self.ts:\n",
    "                self.ts[i] = self.ts[i].fillna(0)\n",
    "        else:\n",
    "            # background will have largest average = minimum across all conditions\n",
    "            min_val = self.clrm[[str(i) for i in range(self.n_conditions)]].min().min()\n",
    "            scale = min_val/np.array([utils.piecewise_fit(min_val, *utils.fit_params)])[0]\n",
    "            for i in tqdm(self.ts):\n",
    "                vals = utils.get_truncated_normal(n = self.ts[i].shape[0]*self.ts[i].shape[1], \n",
    "                                                  sd = noise*min_val, mean = 0)*scale\n",
    "                self.ts[i][:] = vals.reshape(self.ts[i].shape)\n",
    "\n",
    "\n",
    "        for cond in tqdm(self.ts):\n",
    "            for i in self.clrm.index:\n",
    "                avg_val = self.clrm.loc[i, cond]\n",
    "                coords = self.clrm.loc[i, 'ts_coordinates']\n",
    "\n",
    "                self.ts[cond].values[coords] = avg_val if noise == 0 else \\\n",
    "                                          utils.get_truncated_normal(n = len(coords[0]), sd = noise*avg_val, mean = avg_val)\n",
    "\n",
    "        \n",
    "#     def _generate_t0(self, noise, binary):\n",
    "#         '''Generate tensor slice 0. See tensor_slice_t0 method for agrument descriptions'''\n",
    "#         # initialize\n",
    "#         self.ts0 = pd.DataFrame(columns = self.cci.cell_interactions, index = self.LR.edge_list)\n",
    "\n",
    "#         #------------------------------------------------------------------------------------------\n",
    "\n",
    "        \n",
    "\n",
    "#         # background\n",
    "#         if noise == 0:\n",
    "#             self.ts0.fillna(0, inplace = True)\n",
    "#         else:\n",
    "#             if not binary: # generate background noise values\n",
    "#                 # noise values increase with noise, and have a maximum average of self.clrm['mean'].min()\n",
    "#                 scale = self.clrm['mean'].min()/np.array([utils.piecewise_fit(self.clrm['mean'].min(), *utils.fit_params)])[0]\n",
    "#                 vals = utils.get_truncated_normal(n = self.ts0.shape[0]*self.ts0.shape[1], \n",
    "#                                                   sd = noise*self.clrm['mean'].min(), mean = 0)*scale\n",
    "#                 self.ts0[:] = vals.reshape(self.ts0.shape)\n",
    "#             else:\n",
    "#                 raise ValueError('Binary situations not yet dealt with')\n",
    "#         #        freq = np.mean([abs(i) if abs(i) <= 1 else 1 for i in np.random.normal(loc = 0, scale = noise*0.33, size = 10**5)])\n",
    "#         #                 background_coord_noise = random.sample(background_coord, k = int(round(freq*len(background_coord))))\n",
    "#         #                 for coord in background_coord_noise:\n",
    "#         #                     self.ts0.iloc[coord] = 1\n",
    "\n",
    "#         #------------------------------------------------------------------------------------------\n",
    "\n",
    "#         # add CCI values by cell-LR metadata pairs\n",
    "#         for i in self.clrm.index:\n",
    "#             avg_val = self.clrm.loc[i, 'mean']\n",
    "#             coords = self.clrm.loc[i, 'ts_coordinates']\n",
    "#             if not binary: \n",
    "#                 self.ts0.values[coords] = avg_val if noise == 0 else \\\n",
    "#                                           utils.get_truncated_normal(n = len(coords[0]), sd = noise*avg_val, mean = avg_val)\n",
    "#             else:\n",
    "#                 raise ValueError('Binary situations not yet dealt with')\n",
    "#         #         adj_coord = random.sample(coords, int(round(len(coords)*self.clrm.loc[i, 'mean'])))\n",
    "#         #         for coord in adj_coord:\n",
    "#         #             self.ts0.iloc[coord] = 1\n",
    "#         #         if noise > 0:\n",
    "#         #             cm = {0:1, 1:0}\n",
    "#         #             change_coords = random.sample(coords, int(round(len(coords)*self.clrm.loc[i, 'mean']*noise*0.5)))\n",
    "#         #             for coord in change_coords:\n",
    "#         #                 self.ts0.iloc[coord] = cm[self.ts0.iloc[coord]]\n",
    "    \n",
    "#     def tensor_slice_t0(self, noise = 0, n_patterns = 2, binary = False):\n",
    "\n",
    "#         '''Simulates a static time point tensor slice\n",
    "        \n",
    "#         *Note, in current format, only one cell-cell metadata subcategory can have an interaction pattern \n",
    "#         ie, can't combine multiple cell-cell pairs to have the same pattern\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         n_patterns: int (> 0)\n",
    "#             the number of cell metadata - LR metadata pairs for which to form distinct interactions \n",
    "#             the remainder will default to 0, with noise increasing this value\n",
    "#             the groups with distinct interactions will each have distincts values\n",
    "#             recommended to set decomposition rank = n_patterns\n",
    "#         binary: bool\n",
    "#             whether L-R scores are binary or continuous b/w [0,1]\n",
    "#         noise: float [0,1]\n",
    "#             the amount of noise to add to the data\n",
    "#             as noise increases, the fraction of noisy cc-lr coordinates increases, as does the change in value of the interaction, and the standard deviation if non-continuous\n",
    "        \n",
    "#         Returns\n",
    "#         -------\n",
    "#         self.ts0: pd.DataFrame\n",
    "#             matrix with cell network_type pairs as columns, ligand-receptor pairs as rows, scores as entries\n",
    "#         '''\n",
    "        \n",
    "#         if sim.cci is None or type(sim.cci) != CCI_MD:\n",
    "#             raise ValueError('Make sure to generate cell-cell network and metadata with the CCI_MD() class')\n",
    "#         if noise > 1 or noise < 0: \n",
    "#             raise ValueError('Noise must be between 0 and 1')\n",
    "#         if type(binary) is not bool:\n",
    "#             raise ValueError('binary arg must be boolean')\n",
    "        \n",
    "#         # by separating into two methods, can test different values of noise without changing category pairings\n",
    "#         self._generate_clrm(n_patterns = n_patterns)\n",
    "#         self._generate_t0(noise = noise, binary = binary)\n",
    "            \n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hratch/Projects/cci_dt/notebooks/simulation/tmp_57_bao7_bipartite_sf.csv\n",
      "Generate undirected, bipartite, scale-free graph\n",
      "Check network properties\n",
      "All properties are as expected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../scripts/simulation/graphs.py:164: UserWarning: 998 nodes are disconnected, removing from network\n",
      "  warnings.warn(mssg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove homotypic cell interactions for 1 categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hratch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:320: UserWarning: Only continuous scoring is currently implemented\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate background noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:05<00:00,  2.30it/s]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]/home/hratch/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:345: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "100%|██████████| 12/12 [00:00<00:00, 267.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "sim_sf = Simulate() \n",
    "sim_norm = Simulate()\n",
    "\n",
    "# simulate a randomly connected ligand-receptor network (potential interactions)\n",
    "sim_sf.LR_network(network_type = 'scale-free', **{'nodes': 1000, 'degrees': 3, 'alpha': 2}) #scale-free\n",
    "sim_norm.LR_network(network_type = 'normal', **{'n_ligands': 500, 'n_receptors': 500, 'p': 0.5}) # normally distributed\n",
    "# from here on proceed with the scale-free network\n",
    "sim = sim_sf\n",
    "\n",
    "# LR metadata\n",
    "sim.LR.generate_metadata(n_LR_cats = {3: 0}, cat_skew = 0)\n",
    "\n",
    "# cell metadata\n",
    "cci = CCI_MD()\n",
    "cci.cci_network(n_cells = 50, directional = False)\n",
    "# generate 1 metadata categories, with 3 subcategories and 0 skew, the overall skew of categories is 0\n",
    "cci.generate_metadata(n_cell_cats = {3: 0}, cat_skew = 0, remove_homotypic = 1)\n",
    "# add metadata to simulation object\n",
    "sim.cci = cci\n",
    "\n",
    "# generate n_patter metadata groups of CC-LR pairs that change across n_conditions\n",
    "# these changes can either be linear, oscillating, or a pulse\n",
    "sim.generate_tensor_md(n_patterns = 4, n_conditions = 12, patterns = ['pulse', 'linear', 'oscillate'])\n",
    "\n",
    "#generate a t0 tensor slice with continuous LR scores and baseline noise\n",
    "sim.generate_tensor(noise = 0.05, binary = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender are rows, receivers are columns, each slice of the 3d tense is a LR pair, \n",
    "4d tensor is condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cci_dt] *",
   "language": "python",
   "name": "conda-env-cci_dt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
