{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import networkx as nx\n",
    "import igraph\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph_generator():\n",
    "    def __init__(self):\n",
    "        '''Initializer for generating and analyzing various networks'''\n",
    "    \n",
    "    def bipartite_sf(self, nodes, degrees, alpha = 2, edges = None, check_properties = True, compare_barabasi = True):\n",
    "        '''Wraps bigraph_r and retrieves properties from the resultant random, undirected, scale-free, bipartite network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nodes: int\n",
    "            2*n is the number of nodes in graph. Restrictions: number of nodes in group 1 = number of nodes in group 2. \n",
    "        degrees: int\n",
    "            average degree of nodes \n",
    "        alpha: float\n",
    "            scale-free exponent for network degree distribution (recommended 2<alpha<3)\n",
    "        edges: int, optional\n",
    "            number of (directed) edges\n",
    "        check_properties: bool\n",
    "            checks network properties (scale-free power distribution, undirected)\n",
    "        compare_barabasi: bool\n",
    "            checks how closely the bipartite network matched a Barabasi alg generated network with similar input parameters\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B: dict\n",
    "            keys: 'nx' and/or 'ig'\n",
    "            values: networkx and/or igraph objects of the generated random, undirected, scale-free, bipartite network \n",
    "        node_groups: dict\n",
    "            grouping of nodes into ligands (key = 'L') or receptors (key = 'R')\n",
    "        fit: igraph.FittedPowerLaw\n",
    "            scale-free network parameters for B_ig (p-value from Kolmogrov-Smirnov test)\n",
    "        comp: pd.DataFrame or None\n",
    "            summary of differences in network properties between  bipartite network and similar Barabasi network\n",
    "        '''\n",
    "    #     beta: float\n",
    "    #         fitness of node, alpha = 1 / beta + 1, alpha is scale-free exponent\n",
    "    #     beta = (1/alpha)-1\n",
    "\n",
    "        output = tempfile.mkstemp(suffix = '_bipartite_sf.csv', dir = './')[1]\n",
    "#         output = tempfile.mkstemp(suffix = '_bipartite_sf.csv', \n",
    "#                                   dir = 'abspath')[1]\n",
    "        print(output)\n",
    "        beta = alpha\n",
    "        cmd = 'Rscript bigraph_r.r ' + '--beta=' + str(beta) + ' --nodes=' + str(nodes) + ' --degrees=' + str(degrees)\n",
    "        cmd += ' --output=' + str(output) \n",
    "        if edges is not None:\n",
    "            cmd += ' --edges=' + str(edges)\n",
    "\n",
    "        print('Generate undirected, bipartite, scale-free graph')\n",
    "        os.system(cmd)\n",
    "\n",
    "        # format adjacency matrix\n",
    "#         os.chdir(abspath)\n",
    "#         adj_matrix = pd.read_csv(os.path.join('../../scripts/simulation', os.path.basename(output)), index_col = 0)\n",
    "        adj_matrix = pd.read_csv(output, index_col = 0)\n",
    "#         os.chdir(curdir)\n",
    "       \n",
    "        os.remove(output)\n",
    "        adj_matrix.index = adj_matrix.index -1 # 0 indexing\n",
    "        adj_matrix.columns = adj_matrix.index\n",
    "\n",
    "        # extract information\n",
    "        node_groups = {'L': list(range(nodes)), 'R': list(range(nodes, 2*nodes))}\n",
    "\n",
    "        # igraph\n",
    "        A = adj_matrix.replace(0, float('nan'), inplace = False).stack().reset_index()\n",
    "        B_ig = igraph.Graph.Bipartite(edges = list(zip(A.level_0, A.level_1)), \n",
    "                              types = [True]*nodes + [False]*nodes)\n",
    "\n",
    "        fit = self.power_fit(B_ig)\n",
    "        if check_properties:\n",
    "            print('Check network properties')\n",
    "            perfect = True\n",
    "            if fit.p < 0.05:\n",
    "                perfect = False\n",
    "                warnings.warn('Network is not scale-free')\n",
    "\n",
    "            change = alpha/fit.alpha \n",
    "            if (change > 2) or (change < 0.5):\n",
    "                perfect = False\n",
    "                warnings.warn('Power law alpha fit is off by more than 2-fold')\n",
    "\n",
    "            if igraph.Graph.is_directed(B_ig):\n",
    "                perfect = False\n",
    "                warnings.warn('Netork is directed, expected undirected')\n",
    "\n",
    "            if perfect:\n",
    "                print('All properties are as expected')\n",
    "\n",
    "        comp = None\n",
    "        if compare_barabasi:\n",
    "            if isinstance(degrees,float):\n",
    "                degrees = int(round(degrees))\n",
    "            Bt = igraph.Graph.Barabasi(n = B_ig.vcount(), m = degrees, directed = False)\n",
    "            fit_t = self.power_fit(Bt) \n",
    "\n",
    "            comp = pd.DataFrame(data = {'Bipartite': [B_ig.vcount(), B_ig.ecount(), fit.alpha, fit.p ],\n",
    "                         'Barabasi': [Bt.vcount(), Bt.ecount(), fit_t.alpha, fit_t.p ]}, \n",
    "                index = ['nodes', 'edges', 'fitted alpha', 'p-val'])\n",
    "            comp['change (Barabasi/Bipartite)'] = comp.Barabasi/comp.Bipartite\n",
    "        \n",
    "        B = dict({'ig': B_ig})\n",
    "        B_nx = nx.bipartite.from_biadjacency_matrix(scipy.sparse.bsr_matrix(adj_matrix.loc[node_groups['L'], node_groups['R']]))\n",
    "        for (n1, n2, d) in B_nx.edges(data=True): # convert to unweighted\n",
    "            d.clear()\n",
    "        B['nx'] = B_nx\n",
    "        \n",
    "        return B, node_groups, fit, comp\n",
    "            \n",
    "    def nx_to_edgelist(self, B, n):\n",
    "        '''Convert an undirected, binary bipartite network to an edge list.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        B: nx.Graph\n",
    "            an undirected, unweighted, bipartite network. Group 1 nodes are ordered as described in nx.bipartite.random_graph\n",
    "        n: int\n",
    "            the number of nodes in group 1\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B: nx.Graph\n",
    "            same as input, with disconnected nodes removed from network\n",
    "        edge_list: list\n",
    "            each entry is a tuple representing an edge between a nodes in group 1 and a node in group 2\n",
    "        node_groups: dict\n",
    "            keys: '1' for group 1, '2' for group 2\n",
    "            values: lists corresponding to node labels for each group\n",
    "\n",
    "        '''\n",
    "        \n",
    "        if not nx.bipartite.is_bipartite(B) or nx.is_directed(B):\n",
    "            raise ValueError('Network must be bipartite and undirected')\n",
    "        \n",
    "        group1 = list(B.nodes)[:n]\n",
    "        group2 = list(B.nodes)[n:]\n",
    "\n",
    "        disconnected = list(nx.isolates(B))\n",
    "        if len(disconnected) > 0:\n",
    "            mssg = '{} nodes are disconnected, removing from network'.format(len(disconnected))\n",
    "            warnings.warn(mssg)\n",
    "\n",
    "            group1 = sorted(set(group1).difference(disconnected))\n",
    "            group2 = sorted(set(group2).difference(disconnected))\n",
    "\n",
    "            B.remove_nodes_from(disconnected)\n",
    "\n",
    "        node_groups = {'1': group1, '2': group2}\n",
    "        edge_list = list(B.edges)\n",
    "        \n",
    "        return B, edge_list, node_groups\n",
    "    \n",
    "    def drop_disconnected_nodes(self, G):\n",
    "        '''Remove disconnected nodes from network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        B: nx.Graph \n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        B: nx.Graph\n",
    "            same as input but with nodes that have degree = 0 removed\n",
    "        \n",
    "        '''\n",
    "        G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "    def power_fit(self, G):\n",
    "        '''Gets the power law fitted parameters for a graph\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G: nx.Graph or igraph.Graph\n",
    "            Don't have an options for weighted bipartite networkx graphs yet.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        fit: igraph.FittedPowerLaw\n",
    "            scale-free network parameters for B_ig (p-value from Kolmogrov-Smirnov test)\n",
    "        '''\n",
    "        if isinstance(G, nx.Graph):\n",
    "            G_ = self.ig_from_nx(G)\n",
    "        else:\n",
    "            G_ = G.copy()\n",
    "        fit = igraph.power_law_fit(G_.degree())\n",
    "        return fit\n",
    "\n",
    "    def ig_from_nx(self, G):\n",
    "        '''Conversion from networkx object to igraph object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G: nx.Graph \n",
    "            Don't have an options for weighted bipartite networkx graphs yet.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        G_ig: igraph.Graph\n",
    "        '''\n",
    "\n",
    "        if nx.bipartite.is_bipartite(G):\n",
    "            if nx.is_weighted(G):\n",
    "                raise ValueError('No code for converting weighted bipartite networkx graphs to igraph')\n",
    "            # must be continuously sorted for ig\n",
    "            G_ = nx.relabel_nodes(G, mapping = {v:k for k,v in dict(enumerate(G.nodes)).items()}, copy = True)\n",
    "            groups = nx.bipartite.sets(G_)\n",
    "            mapper = {k: True for k in groups[0]}\n",
    "            mapper.update({k: False for k in groups[1]})\n",
    "            mapper = OrderedDict({k: mapper[k] for k in sorted(mapper)})\n",
    "            G_ig = igraph.Graph.Bipartite(types = list(mapper.values()), edges = sorted(G_.edges), \n",
    "                                         directed = nx.is_directed(G_))\n",
    "            for new, old in dict(enumerate(G.nodes)).items():\n",
    "                G_ig.vs[new]['id'] = old\n",
    "        else:\n",
    "            mode = 'DIRECTED' if nx.is_directed(G) else 'UNDIRECTED'\n",
    "            G_ig = igraph.Graph.Adjacency((nx.to_numpy_matrix(G) > 0).tolist(), mode = mode)    \n",
    "        return G_ig\n",
    "    \n",
    "    def summarize(self, G):\n",
    "        '''Generate network level summary statistics\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        G: nx.Graph\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        summary: pd.DataFrame\n",
    "            alpha is fitted the power law exponent\n",
    "            p-value is from the Kolmogorov-Smignrov test for power-law\n",
    "            remainder are network level statistics\n",
    "        \n",
    "        '''\n",
    "        summary = pd.DataFrame(index = ['alpha', 'p-val', 'clustering_coefficient', \n",
    "                                  'median_degree', 'median_eigenvector_centrality'])\n",
    "        try:\n",
    "            fit = self.power_fit(G)\n",
    "            alpha, p = fit.alpha, fit.p\n",
    "        except:\n",
    "            alpha, p = float('nan'), float('nan')\n",
    "        \n",
    "        C = nx.average_clustering(G)\n",
    "        \n",
    "        try:\n",
    "            centrality = np.median(list(nx.centrality.eigenvector.eigenvector_centrality(G).values()))\n",
    "        except:\n",
    "            centrality = float('nan')\n",
    "\n",
    "        summary[''] = [alpha,p, C, np.median([i[1] for i in G.degree]), centrality]\n",
    "        return summary\n",
    "    \n",
    "    def subset_nodes(self, G, nodes = None, subset_size = None, drop = True):\n",
    "        '''Generate a network subset by randomly removing nodes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G: nx.Graph \n",
    "        nodes: list\n",
    "            Each entry is a node in G to be removed, takes precedence of subset_size\n",
    "        subset_size: float (0,1)\n",
    "            removes a random subset, leaving a network with size proprtional to subset_size*G\n",
    "        drop: bool\n",
    "            whether to drop disconnected nodes\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        _G: nx.Graph\n",
    "            same as input but with specified nodes removed\n",
    "\n",
    "        '''\n",
    "        _G = G.copy()\n",
    "        if nodes is None:\n",
    "            if subset_size is None or subset_size >= 1 or subset_size <= 0:\n",
    "                raise ValueError('Must specify an appropriate subset size when nodes is not specified')\n",
    "            nodes = random.sample(G.nodes, round(len(G.nodes)*(1-subset_size)))\n",
    "        \n",
    "        _G.remove_nodes_from(nodes)\n",
    "        if drop:\n",
    "            self.drop_disconnected_nodes(_G)\n",
    "        return _G\n",
    "    def subset_edges(self, G, edges = None, subset_size = None, drop = True):\n",
    "        '''Generate a network subset by randomly removing edges\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        G: nx.Graph \n",
    "        edges: list\n",
    "            Each entry is an edge in G to be removed, takes precedence of subset_size\n",
    "        subset_size: float (0,1)\n",
    "            removes a random subset, leaving a network with size proprtional to subset_size*G\n",
    "        drop: bool\n",
    "            whether to drop disconnected nodes\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        _G: nx.Graph\n",
    "            same as input but with specified edges removed\n",
    "\n",
    "        '''\n",
    "        _G = G.copy()\n",
    "        if edges is None:\n",
    "            if subset_size is None or subset_size >= 1 or subset_size <= 0:\n",
    "                raise ValueError('Must specify an appropriate subset size when nodes is not specified')\n",
    "            edges = random.sample(G.edges, round(len(G.edges)*(1-subset_size)))\n",
    "\n",
    "        _G.remove_edges_from(edges)\n",
    "        if drop:\n",
    "            self.drop_disconnected_nodes(_G)\n",
    "        return _G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = graph_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hratch/Projects/cci_dt/notebooks/simulation/tmplffv128e_bipartite_sf.csv\n",
      "Generate undirected, bipartite, scale-free graph\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-df48ee1d9344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipartite_sf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-60ea1b2f3593>\u001b[0m in \u001b[0;36mbipartite_sf\u001b[0;34m(self, nodes, degrees, alpha, edges, check_properties, compare_barabasi)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#         os.chdir(abspath)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#         adj_matrix = pd.read_csv(os.path.join('../../scripts/simulation', os.path.basename(output)), index_col = 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0madj_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;31m#         os.chdir(curdir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cci_dt/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cci_dt/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cci_dt/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cci_dt/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cci_dt/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "output = tempfile.mkstemp(suffix = '_bipartite_sf.csv', dir = './')[1]\n",
    "#         output = tempfile.mkstemp(suffix = '_bipartite_sf.csv', \n",
    "#                                   dir = 'abspath')[1]\n",
    "print(output)\n",
    "beta = alpha\n",
    "cmd = 'Rscript bigraph_r.r ' + '--beta=' + str(beta) + ' --nodes=' + str(nodes) + ' --degrees=' + str(degrees)\n",
    "cmd += ' --output=' + str(output) \n",
    "if edges is not None:\n",
    "    cmd += ' --edges=' + str(edges)\n",
    "\n",
    "print('Generate undirected, bipartite, scale-free graph')\n",
    "os.system(cmd)\n",
    "\n",
    "# format adjacency matrix\n",
    "#         os.chdir(abspath)\n",
    "#         adj_matrix = pd.read_csv(os.path.join('../../scripts/simulation', os.path.basename(output)), index_col = 0)\n",
    "adj_matrix = pd.read_csv(output, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cci_dt] *",
   "language": "python",
   "name": "conda-env-cci_dt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
